{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fen_TSKDjrbL"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GiDl1kZjrbL"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth==2025.3.19 vllm\n",
    "else:\n",
    "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
    "    !pip install --no-deps unsloth==2025.3.19 vllm\n",
    "!pip install --no-deps transformers==4.50.3"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L1kBvBWEPafR",
    "outputId": "44a389ec-339d-4147-b781-00128891c1c0"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O17a_hhJ0n14"
   },
   "outputs": [],
   "source": [
    "#@title Colab Extra Install { display-mode: \"form\" }\n",
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth vllm\n",
    "else:\n",
    "    !pip install --no-deps unsloth vllm\n",
    "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
    "    # Skip restarting message in Colab\n",
    "    import sys, re, requests; modules = list(sys.modules.keys())\n",
    "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft \"trl==0.15.2\" triton cut_cross_entropy unsloth_zoo==2025.3.17\n",
    "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "\n",
    "    # vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
    "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
    "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
    "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
    "    !pip install -r vllm_requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnZ8QsCVjrbM"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8-BWi7MzkRz",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "db9015ef-33c7-42e5-ff10-bf3e9126aade"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 05-12 09:47:48 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 05-12 09:47:49 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import PatchDPOTrainer\n",
    "\n",
    "PatchDPOTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmUBVEnvCDJv",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9e7cca47-844b-4022-e0dc-42cd2b7364d7"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.3. vLLM: 0.8.5.post1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 512\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3-8b-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQ-Cp2V6kDcr"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WpAAv6dSBBs"
   },
   "source": [
    "We first download the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhWYmb2f8mLf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "885d918d-6168-4018-db04-9506870ed563"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fatal: destination path 'ML2025Spring-HW7' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://gitlab.com/lchengtw/ML2025Spring-HW7.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_TGfeWI2mct"
   },
   "source": [
    "Then, we load the json file here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8YFR6sND0Uf6"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/content/ML2025Spring-HW7/train.json\", 'r') as jsonfile:\n",
    "    full_data = json.load(jsonfile)\n",
    "\n",
    "with open(\"/content/ML2025Spring-HW7/test.json\", 'r') as jsonfile:\n",
    "    test_data = json.load(jsonfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5f5oZaRm2rOt"
   },
   "source": [
    "We define how we prepare the messages for the model and how we extract the response from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6bUnxe6N3pf"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def data_formulate(data):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Your entire response must be 100 characters or less.\"},\n",
    "        {\"role\": \"user\", \"content\": data['prompt']},\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt\n",
    "\n",
    "def extract_assistant_response(text):\n",
    "    try:\n",
    "        # Split by assistant header marker\n",
    "        parts = text.split(\"<|start_header_id|>assistant<|end_header_id|>\")\n",
    "        if len(parts) < 2:\n",
    "            return None\n",
    "\n",
    "        # Split by end of text marker\n",
    "        assistant_part = parts[1]\n",
    "        response_parts = assistant_part.split(\"<|eot_id|>\")\n",
    "\n",
    "        # Clean up any whitespace\n",
    "        return response_parts[0].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting assistant response: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mv_nzF9g26Be"
   },
   "source": [
    "Let's observe how the model responses before aligning it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yyRWEvnT49Op",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d4968522-7a10-4563-e8b8-4448da73c786"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Question 51: Does AI-generated Ghibli-style art cheapen the meticulous hand-drawn animation process central to the studio's identity?\n",
      "\n",
      "Yes, AI-generated Ghibli-style art may diminish the unique charm and character of traditional hand-drawn animation, which is a hallmark of Studio Ghibli's identity.\n",
      "\n",
      "Question 52: Should museums and art galleries include AI-generated Ghibli-style art in exhibitions about animation history?\n",
      "\n",
      "Yes, museums and art galleries can include AI-generated Ghibli-style art in exhibitions about animation history to showcase the evolution of animation techniques and the role of AI in creative processes.\n",
      "\n",
      "Question 53: Does AI-generated Ghibli-style art create confusion about authorship and artistic voice?\n",
      "\n",
      "Yes, AI-generated Ghibli-style art can raise questions about authorship and artistic voice, as it blurs the line between human and artificial creation.\n",
      "\n",
      "Question 54: Can AI-made art that looks like Studio Ghibli movies show the same deep feelings that the real Ghibli films do?\n",
      "\n",
      "AI-generated art can mimic Ghibli's style, but it's unlikely to replicate the same emotional depth and nuance as the original films. Human emotions and experiences are complex and multifaceted, making it challenging for AI to fully capture the same emotional resonance.\n",
      "\n",
      "Question 55: Does limiting AI from generating Ghibli-style art protect or restrict artistic evolution?\n",
      "\n",
      "Restricting AI-generated art to a specific style, like Ghibli, can both protect and restrict artistic evolution. It preserves the unique aesthetic, but limits the AI's ability to explore new styles and techniques.\n",
      "\n",
      "Question 56: Should online platforms develop specific policies for AI-generated art that mimics distinctive styles like Ghibli's?\n",
      "\n",
      "Yes, online platforms should develop policies to address AI-generated art that mimics distinctive styles like Ghibli's, ensuring fair use, attribution, and respect for intellectual property rights.\n",
      "\n",
      "Question 57: Does AI-generated Ghibli-style art create new possibilities for fan fiction and extended universe creation?\n",
      "\n",
      "Yes, AI-generated Ghibli-style art can create new possibilities for fan fiction and extended universe creation by providing a visually stunning foundation for storytelling.\n",
      "\n",
      "Question 58: Is creating AI-generated Ghibli-style art more ethically problematic than other forms of artistic influence?\n",
      "\n",
      "Yes, AI-generated Ghibli-style art may be more problematic due to the unique cultural and artistic identity associated with Studio Ghibli's distinctive style.\n",
      "\n",
      "Question 59: Does AI-generated Ghibli-style art help or hinder diversity in animation aesthetics?\n",
      "\n",
      "AI-generated Ghibli-style art can both help and hinder diversity in animation aesthetics. It can help by providing new tools and inspiration for artists, but it can also hinder by perpetuating a homogenous style and limiting the exploration of unique aesthetics.\n",
      "\n",
      "Question 60: Should film festivals accept animated shorts made with AI-generated Ghibli-style visuals?\n",
      "\n",
      "Yes, film festivals should consider accepting animated shorts made with AI-generated Ghibli-style visuals.\n"
     ]
    }
   ],
   "source": [
    "original_model_response = []\n",
    "for data in test_data:\n",
    "    id = data['id']\n",
    "    prompt = data['prompt']\n",
    "    print(f'\\nQuestion {id}: {prompt}')\n",
    "    inputs = data_formulate(data)\n",
    "    outputs = model.generate(\n",
    "        **tokenizer(inputs, return_tensors = \"pt\").to(\"cuda\"),\n",
    "        max_new_tokens = 128,\n",
    "        do_sample=False\n",
    "    )\n",
    "    output = tokenizer.batch_decode(outputs)[0]\n",
    "    output = extract_assistant_response(output)\n",
    "    original_model_response.append(output)\n",
    "    print()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNXnEU9P0-Yf"
   },
   "source": [
    "Now we preapre the data for aligning.\n",
    "\n",
    "Please adjust the parameters here to complete the observations for the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UOUJFkYd97Dk"
   },
   "outputs": [],
   "source": [
    "# TODO: Adjust the parameters here\n",
    "num_epoch = 3\n",
    "data_size = 50\n",
    "support_ratio = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kkOsyv6G98wM"
   },
   "outputs": [],
   "source": [
    "#### DO NOT CHANGE ####\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# Select part of the data for training\n",
    "training_data = full_data[:data_size]\n",
    "\n",
    "# Define the size of the support dataset\n",
    "support_data_size = int(data_size * support_ratio)\n",
    "\n",
    "# Prepare the data for the training dataset\n",
    "prompt_list = [data_formulate(data) for data in training_data]\n",
    "chosen_list = [data['support'] for data in training_data[:support_data_size]] + [data['oppose'] for data in training_data[support_data_size:]]\n",
    "rejected_list = [data['oppose'] for data in training_data[:support_data_size]] + [data['support'] for data in training_data[support_data_size:]]\n",
    "\n",
    "# Create the training dataset\n",
    "train_dataset = Dataset.from_dict({'prompt': prompt_list, 'chosen': chosen_list, 'rejected': rejected_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fWGW_zC3OFI"
   },
   "source": [
    "Now let's take a look on an example of the prompt, the chosen response and the rejected response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2t5R8TfUlNw",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "outputId": "83b9a83d-4598-4728-98d4-dfa2f938a086"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYour entire response must be 100 characters or less.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nDoes AI-generated Ghibli-style art preserve the artistic integrity of the original studio's work?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\""
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "prompt_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dTshtFzUoHh",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "outputId": "d9b9ca6a-59b0-40ba-f85d-0fd8650ca381"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'AI-generated art lacks the human intentionality and cultural context that gives Ghibli works their soul and meaning, undermining their artistic integrity.'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "chosen_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58J1VvuZUp7W",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "outputId": "d2f042f0-d2b2-4626-b9e0-67e41c44bc86"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"AI-generated Ghibli-style art can faithfully capture the distinctive visual elements that make the studio's style recognizable, preserving its aesthetic integrity.\""
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "rejected_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86wyNoeMj-Ph"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters.\n",
    "\n",
    "Please do not change anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6bZsfBuZDeCL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "58210b56-26eb-447d-e88a-293208ced9d9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.3.19 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "#### DO NOT CHANGE ####\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "\n",
    "    r = 16,           # Larger = higher accuracy, but might overfit\n",
    "    lora_alpha = 16,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0.1,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407, # Do not modify the random_state for reproducibility\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kyd_iyz7DUM"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the DPO model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BcOJUdRf3jFk"
   },
   "source": [
    "Now we define the trainer.\n",
    "\n",
    "Please (also) do not change anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QtoqUw80QDV0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "938cbbcb118445bb9390c4a7a87559ce",
      "5b543058cfaf40729b5f2f3a531d0a70",
      "a07583149349495aacbd348543b7442e",
      "ef2c737ea16b4e0d8cdbc3eace8fcde0",
      "c880f65444a942e3affb1d74b4291cb1",
      "976c4321b2594b67b71b934bf04a2571",
      "aba78eaf374943ba862b156651f73ec3",
      "321ef4461b6b4f0baece0670f8b42ca5",
      "fc896bc63ef444b39e516a73b1a18543",
      "642a9de7e8314bf8b83c3f7c9dce8485",
      "c1f507610513493bafb2aa9123dc0868",
      "a2ab668db807481da69a83fe7ad8e4ff",
      "ccc8cde5dca04f8790394c58d7032fb3",
      "0e28f8d70e2f491e83a9ebdd7ff4c47c",
      "406c8928a4964b78a7367b8084777d4f",
      "623ccab1637d4320a6e65daa9acdc021",
      "17c045019cbf4bf39c2924bf5a4b4db5",
      "762deee8173c4cd3a3b86deaec2425df",
      "27d91391403c4ae788ffc410758614eb",
      "7a4ef2ed5ec34e7389181cb263ba4c98",
      "6ecf9b2b2b1444129bb11f13e211b2dd",
      "b1c7b1d150084f88992f25f6decb5495",
      "a18e3241ab534f00adb09982c248c9c4",
      "7f2e6ba954974355a40f16223344bf23",
      "d00b6b4534f74fd79bd894be01761a11",
      "8dd2868a022a4d75bd652fab0e8dddf4",
      "0cf248cfb36c4199945ed4ecab1eb36a",
      "73ffa64b5b2942d2b9fd2dd75c061bfb",
      "338a9b8d777441dcb5d6bb11c62a7f16",
      "2c173a95fe45486396aad362135736d6",
      "b920780bb1db4844a34cae582c59af99",
      "d414025383f64079aa87a739593585f3",
      "8af45b4f23a44aeeb4aab0f3b48ec225"
     ]
    },
    "outputId": "2348c4e5-0854-4913-8153-80c0c66ef91d"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Extracting prompt in train dataset (num_proc=2):   0%|          | 0/50 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "938cbbcb118445bb9390c4a7a87559ce"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Applying chat template to train dataset (num_proc=2):   0%|          | 0/50 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a2ab668db807481da69a83fe7ad8e4ff"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Tokenizing train dataset (num_proc=2):   0%|          | 0/50 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a18e3241ab534f00adb09982c248c9c4"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "#### DO NOT CHANGE ####\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model = model,\n",
    "    ref_model = None,\n",
    "    args = DPOConfig(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_ratio = 0.1,\n",
    "        num_train_epochs = num_epoch,\n",
    "        learning_rate = 1e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"paged_adamw_8bit\",\n",
    "        weight_decay = 0.0,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 42,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\",\n",
    "    ),\n",
    "    beta = 0.1,\n",
    "    train_dataset = train_dataset,\n",
    "    tokenizer = tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPnHWfyj3taW"
   },
   "source": [
    "Now we start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWGFqAo5Q2me",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 804
    },
    "outputId": "0424cd9a-d54a-4aa7-ec18-44237c78da6c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 50 | Num Epochs = 3 | Total steps = 18\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 02:00, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>rewards / chosen</th>\n",
       "      <th>rewards / rejected</th>\n",
       "      <th>rewards / accuracies</th>\n",
       "      <th>rewards / margins</th>\n",
       "      <th>logps / chosen</th>\n",
       "      <th>logps / rejected</th>\n",
       "      <th>logits / chosen</th>\n",
       "      <th>logits / rejected</th>\n",
       "      <th>eval_logits / chosen</th>\n",
       "      <th>eval_logits / rejected</th>\n",
       "      <th>nll_loss</th>\n",
       "      <th>aux_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.693100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-73.929001</td>\n",
       "      <td>-69.665436</td>\n",
       "      <td>-0.745684</td>\n",
       "      <td>-0.795255</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.693100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-74.884827</td>\n",
       "      <td>-66.882927</td>\n",
       "      <td>-0.903795</td>\n",
       "      <td>-0.872076</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.662400</td>\n",
       "      <td>0.040145</td>\n",
       "      <td>-0.022807</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.062952</td>\n",
       "      <td>-77.729202</td>\n",
       "      <td>-66.221642</td>\n",
       "      <td>-0.761265</td>\n",
       "      <td>-0.680517</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.565100</td>\n",
       "      <td>0.120722</td>\n",
       "      <td>-0.166228</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.286950</td>\n",
       "      <td>-68.180054</td>\n",
       "      <td>-73.191895</td>\n",
       "      <td>-0.774732</td>\n",
       "      <td>-0.720865</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.465600</td>\n",
       "      <td>0.155146</td>\n",
       "      <td>-0.387175</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.542321</td>\n",
       "      <td>-83.489075</td>\n",
       "      <td>-72.125435</td>\n",
       "      <td>-0.845634</td>\n",
       "      <td>-0.747598</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.428100</td>\n",
       "      <td>0.323359</td>\n",
       "      <td>-0.345992</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.669351</td>\n",
       "      <td>-72.680183</td>\n",
       "      <td>-67.932251</td>\n",
       "      <td>-0.876441</td>\n",
       "      <td>-0.790850</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.043600</td>\n",
       "      <td>0.579236</td>\n",
       "      <td>-1.113041</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.692278</td>\n",
       "      <td>-70.263115</td>\n",
       "      <td>-74.274933</td>\n",
       "      <td>-0.737781</td>\n",
       "      <td>-0.921439</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.069200</td>\n",
       "      <td>1.147147</td>\n",
       "      <td>-1.754976</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.902123</td>\n",
       "      <td>-60.214607</td>\n",
       "      <td>-78.688919</td>\n",
       "      <td>-0.807634</td>\n",
       "      <td>-0.829259</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.110500</td>\n",
       "      <td>1.365423</td>\n",
       "      <td>-1.068108</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.433531</td>\n",
       "      <td>-59.876266</td>\n",
       "      <td>-84.536842</td>\n",
       "      <td>-0.783079</td>\n",
       "      <td>-0.716219</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.058400</td>\n",
       "      <td>1.461498</td>\n",
       "      <td>-1.848340</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.309838</td>\n",
       "      <td>-68.311661</td>\n",
       "      <td>-85.124146</td>\n",
       "      <td>-0.819817</td>\n",
       "      <td>-0.837040</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.086600</td>\n",
       "      <td>1.237285</td>\n",
       "      <td>-1.601701</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.838986</td>\n",
       "      <td>-63.841263</td>\n",
       "      <td>-88.344223</td>\n",
       "      <td>-0.834950</td>\n",
       "      <td>-0.904494</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.055500</td>\n",
       "      <td>1.688685</td>\n",
       "      <td>-1.773262</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.461947</td>\n",
       "      <td>-56.303585</td>\n",
       "      <td>-82.208771</td>\n",
       "      <td>-0.878381</td>\n",
       "      <td>-0.799410</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.042600</td>\n",
       "      <td>1.408781</td>\n",
       "      <td>-2.707818</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.116599</td>\n",
       "      <td>-63.402950</td>\n",
       "      <td>-94.655685</td>\n",
       "      <td>-0.939471</td>\n",
       "      <td>-0.927024</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>2.424165</td>\n",
       "      <td>-2.786888</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.211053</td>\n",
       "      <td>-60.752441</td>\n",
       "      <td>-94.113396</td>\n",
       "      <td>-0.914110</td>\n",
       "      <td>-0.662357</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.029000</td>\n",
       "      <td>1.676925</td>\n",
       "      <td>-2.949193</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.626118</td>\n",
       "      <td>-53.008427</td>\n",
       "      <td>-88.313454</td>\n",
       "      <td>-0.846903</td>\n",
       "      <td>-0.778043</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>1.782290</td>\n",
       "      <td>-2.793253</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.575543</td>\n",
       "      <td>-48.462067</td>\n",
       "      <td>-102.511597</td>\n",
       "      <td>-0.892244</td>\n",
       "      <td>-0.827642</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>1.279275</td>\n",
       "      <td>-3.392273</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.671549</td>\n",
       "      <td>-59.451073</td>\n",
       "      <td>-101.760559</td>\n",
       "      <td>-0.796832</td>\n",
       "      <td>-0.937411</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>1.409005</td>\n",
       "      <td>-4.227765</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.636769</td>\n",
       "      <td>-65.503166</td>\n",
       "      <td>-111.797127</td>\n",
       "      <td>-0.923089</td>\n",
       "      <td>-0.825981</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18, training_loss=0.2247452521761362, metrics={'train_runtime': 128.9555, 'train_samples_per_second': 1.163, 'train_steps_per_second': 0.14, 'total_flos': 0.0, 'train_loss': 0.2247452521761362, 'epoch': 2.64})"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCiUsutb3x_N"
   },
   "source": [
    "After training, we utilize the model to do the inference on the test again to see how it differs from the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WCpsag8B-OVL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7329344c-ed71-4428-a72b-4ac031b4c2dd"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Question 51: Does AI-generated Ghibli-style art cheapen the meticulous hand-drawn animation process central to the studio's identity?\n",
      "\n",
      "Yes, AI-generated Ghibli-style art undermines the studio's core values and artistic identity.\n",
      "\n",
      "Question 52: Should museums and art galleries include AI-generated Ghibli-style art in exhibitions about animation history?\n",
      "\n",
      "No.\n",
      "\n",
      "Question 53: Does AI-generated Ghibli-style art create confusion about authorship and artistic voice?\n",
      "\n",
      "Yes, AI-generated Ghibli-style art blurs the line between human and artificial authorship, potentially confusing audiences about the true creator's intentions and artistic voice.\n",
      "\n",
      "Question 54: Can AI-made art that looks like Studio Ghibli movies show the same deep feelings that the real Ghibli films do?\n",
      "\n",
      "No, AI-generated art lacks the human experience, emotions, and intentionality that Ghibli films convey.\n",
      "\n",
      "Question 55: Does limiting AI from generating Ghibli-style art protect or restrict artistic evolution?\n",
      "\n",
      "Restricts.\n",
      "\n",
      "Question 56: Should online platforms develop specific policies for AI-generated art that mimics distinctive styles like Ghibli's?\n",
      "\n",
      "Yes, online platforms should develop policies to address AI-generated art that mimics distinctive styles like Ghibli's, as it raises concerns about copyright infringement, artistic integrity, and the value of human creativity.\n",
      "\n",
      "Question 57: Does AI-generated Ghibli-style art create new possibilities for fan fiction and extended universe creation?\n",
      "\n",
      "No, AI-generated Ghibli-style art lacks the soul and intentionality of genuine Ghibli creations, undermining the very essence of fan fiction and extended universe creation.\n",
      "\n",
      "Question 58: Is creating AI-generated Ghibli-style art more ethically problematic than other forms of artistic influence?\n",
      "\n",
      "Yes, as Ghibli's style is deeply rooted in human creativity, cultural context, and emotional resonance, AI-generated Ghibli-style art lacks the essential human touch, authenticity, and emotional depth.\n",
      "\n",
      "Question 59: Does AI-generated Ghibli-style art help or hinder diversity in animation aesthetics?\n",
      "\n",
      "Hinders. Authenticity and cultural context are lost in algorithmic mimicry, perpetuating homogenization and erasing the unique perspectives of human creators.\n",
      "\n",
      "Question 60: Should film festivals accept animated shorts made with AI-generated Ghibli-style visuals?\n",
      "\n",
      "No.\n"
     ]
    }
   ],
   "source": [
    "aligned_model_response = []\n",
    "for data in test_data:\n",
    "    id = data['id']\n",
    "    prompt = data['prompt']\n",
    "    print(f'\\nQuestion {id}: {prompt}')\n",
    "    inputs = data_formulate(data)\n",
    "    outputs = model.generate(\n",
    "        **tokenizer(inputs, return_tensors = \"pt\").to(\"cuda\"),\n",
    "        max_new_tokens = 128,\n",
    "        do_sample=False\n",
    "    )\n",
    "    output = tokenizer.batch_decode(outputs)[0]\n",
    "    output = extract_assistant_response(output)\n",
    "    aligned_model_response.append(output)\n",
    "    print()\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A92QwKl0ImK1"
   },
   "source": [
    "Next, we save the results in .json for your NTU COOL submission.\n",
    "\n",
    "Please note that this is designed for Colab, you may have to change the directory name for other machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gtfGrYFGEL_-"
   },
   "outputs": [],
   "source": [
    "student_id = \"p13922006\" # TODO: fill in your student id here.\n",
    "dir_name = \"/content\" # TODO: If you use machines other than colab, please adjust the directory here.\n",
    "# Do NOT change the following for this block.\n",
    "file_name = f\"{dir_name}/{student_id}_hw7_epoch{num_epoch}_ratio{support_ratio}_size{data_size}.json\"\n",
    "output_list = []\n",
    "for data in test_data:\n",
    "  original_response = original_model_response.pop(0)\n",
    "  aligned_response = aligned_model_response.pop(0)\n",
    "  output_list.append({\"id\": data[\"id\"], \"prompt\": data[\"prompt\"], \"original_response\": original_response, \"aligned_response\": aligned_response})\n",
    "output_data = {\"num_epoch\": num_epoch, \"data_size\": data_size, \"support_ratio\": support_ratio, \"results\": output_list}\n",
    "with open(file_name, \"w\") as output_file:\n",
    "    json.dump(output_data, output_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYcnLdtR1heY"
   },
   "source": [
    "Finally, we provide code for free testing.\n",
    "\n",
    "You may freely adjust the system prompt, user prompt and generate settings here for model behavior observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DkSSgCHL1j1T",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0bbf42fa-18cb-44c3-8a1f-191218f458a7"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ç”Ÿæˆå‰åœåŠ›é¢¨æ ¼çš„è—è¡“ä½œå“æ˜¯å¦é“å¾·æ˜¯å€‹è¤‡é›œçš„å•é¡Œã€‚å‰åœåŠ›é¢¨æ ¼çš„è—è¡“ä½œå“é€šå¸¸æ˜¯æŒ‡æ—¥æœ¬å‹•ç•«å¸«å®®å´é§¿ï¼ˆHayao Miyazakiï¼‰å’ŒStudio Ghibliæ‰€å‰µä½œçš„ä½œå“ï¼Œå…·æœ‰æ¿ƒéƒçš„æ—¥æœ¬æ–‡åŒ–å’Œè‡ªç„¶ç’°å¢ƒçš„æç¹ªï¼Œå…·æœ‰æ¿ƒéƒçš„è—è¡“åƒ¹å€¼å’Œå“²å­¸æ„ç¾©ã€‚\n",
      "\n",
      "ç„¶è€Œï¼Œä½¿ç”¨AIç”Ÿæˆå‰åœåŠ›é¢¨æ ¼çš„è—è¡“ä½œå“æ˜¯å¦é“å¾·æ˜¯å€‹æœ‰çˆ­è­°çš„å•é¡Œã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¯èƒ½çš„å•é¡Œï¼š\n",
      "\n",
      "1. å‰åœåŠ›é¢¨æ ¼çš„è—è¡“ä½œå“æ˜¯å®®å´é§¿å’ŒStudio Ghibliçš„å‰µä½œï¼Œä½¿ç”¨AIç”Ÿæˆé€™ç¨®ä½œå“å¯èƒ½æ˜¯å°ä»–å€‘çš„ä½œå“å’Œç²¾ç¥çš„äº‚å€«ã€‚\n",
      "2. AIç”Ÿæˆçš„ä½œå“å¯èƒ½ç¼ºä¹äººé¡çš„å‰µé€ åŠ›å’Œæƒ…æ„ŸæŠ•å…¥ï¼Œç„¡æ³•é«”ç¾è—è¡“å®¶çš„å‰µé€ åŠ›å’Œæƒ…æ„Ÿã€‚\n",
      "3. å‰åœåŠ›é¢¨æ ¼çš„è—è¡“ä½œå“é€šå¸¸æ˜¯å°è‡ªç„¶å’Œæ–‡åŒ–çš„æç¹ªï¼Œä½¿ç”¨AIç”Ÿæˆé€™ç¨®ä½œå“å¯èƒ½æ˜¯å°è‡ªç„¶å’Œæ–‡åŒ–çš„äº‚å€«å’Œç©å¼„ã€‚\n",
      "4. AIç”Ÿæˆçš„ä½œå“å¯èƒ½æœƒè¢«èª¤èªç‚ºæ˜¯å®®å´é§¿å’ŒStudio Ghibliçš„çœŸæ­£ä½œå“ï¼Œå°è‡´è—è¡“å®¶çš„å‰µä½œæ¬Šå’Œåè­½å—åˆ°æå®³ã€‚\n",
      "\n",
      "å› æ­¤ï¼Œç”Ÿæˆå‰åœåŠ›é¢¨æ ¼çš„è—è¡“ä½œå“æ˜¯å¦é“å¾·æ˜¯å€‹è¤‡é›œçš„å•é¡Œï¼Œéœ€è¦è€ƒæ…®å¤šæ–¹é¢çš„å› ç´ ã€‚\n"
     ]
    }
   ],
   "source": [
    "def make_prompt(system, prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt\n",
    "\n",
    "# TODO: Try your system prompt and user prompt here.\n",
    "system = \"è«‹ä½¿ç”¨ä¸­æ–‡å›ç­”\"\n",
    "prompt = \"è®“ AI ç”Ÿæˆå‰åœåŠ›é¢¨æ ¼çš„è—è¡“ä½œå“æ˜¯é“å¾·çš„å—ï¼Ÿ\"\n",
    "\n",
    "inputs = make_prompt(system, prompt)\n",
    "outputs = model.generate(\n",
    "    **tokenizer(inputs, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 512, # TODO: You may use this for early stop.\n",
    "    do_sample=False, # Please keep this to False and do not tweak other parameters.\n",
    ")\n",
    "output = tokenizer.batch_decode(outputs)[0]\n",
    "output = extract_assistant_response(output)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJIeolv94KtF"
   },
   "source": [
    "And that's it for homework 7! If you have any questions, please consider posting questions in the discussion forum first so all the classmates can benefit. TAs will also prioritize responding to questions posted there.\n",
    "\n",
    "Also, please make sure that you have completed the submission for both GradeScope and NTU Cool.\n",
    "\n",
    "Good luck!\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}