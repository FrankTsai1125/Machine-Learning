{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCAjaBfgPnPQ"
   },
   "source": [
    "# ML2025 Homework 10 - Diffusion\n",
    "\n",
    "This notebook is for ML2025 Homework 10, focusing on the customization of diffusion model. The goal is to use BLIP Diffusion and Custom Diffusion to generate images with custom objects.\n",
    "\n",
    "Codes in this notebook is modified from [ref1](https://huggingface.co/docs/diffusers/en/api/pipelines/blip_diffusion) and [ref2](https://huggingface.co/docs/diffusers/en/training/custom_diffusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHDz9O9cQCl8"
   },
   "source": [
    "# Utilitites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "finzPpwQQzIu"
   },
   "source": [
    "## Check Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wlhrT1E_Q04c",
    "outputId": "8c576f86-a759-47c3-bf6d-5f2522833281"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Thu Jun 12 09:15:54 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   56C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "Collecting transformers==4.50.3\n",
      "  Using cached transformers-4.50.3-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting filelock (from transformers==4.50.3)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers==4.50.3)\n",
      "  Using cached huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy>=1.17 (from transformers==4.50.3)\n",
      "  Using cached numpy-2.3.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting packaging>=20.0 (from transformers==4.50.3)\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pyyaml>=5.1 (from transformers==4.50.3)\n",
      "  Using cached PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.50.3)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers==4.50.3)\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.50.3)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers==4.50.3)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers==4.50.3)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.3)\n",
      "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.3)\n",
      "  Using cached typing_extensions-4.14.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.3)\n",
      "  Using cached hf_xet-1.1.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->transformers==4.50.3)\n",
      "  Using cached charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers==4.50.3)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers==4.50.3)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers==4.50.3)\n",
      "  Using cached certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Using cached transformers-4.50.3-py3-none-any.whl (10.2 MB)\n",
      "Using cached huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "Using cached numpy-2.3.0-cp311-cp311-manylinux_2_28_x86_64.whl (16.9 MB)\n",
      "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Using cached PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n",
      "Using cached regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Using cached charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (147 kB)\n",
      "Using cached fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Using cached hf_xet-1.1.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached typing_extensions-4.14.0-py3-none-any.whl (43 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, tqdm, safetensors, regex, pyyaml, packaging, numpy, idna, hf-xet, fsspec, filelock, charset_normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.4.0\n",
      "    Uninstalling urllib3-2.4.0:\n",
      "      Successfully uninstalled urllib3-2.4.0\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.14.0\n",
      "    Uninstalling typing_extensions-4.14.0:\n",
      "      Successfully uninstalled typing_extensions-4.14.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.67.1\n",
      "    Uninstalling tqdm-4.67.1:\n",
      "      Successfully uninstalled tqdm-4.67.1\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.5.3\n",
      "    Uninstalling safetensors-0.5.3:\n",
      "      Successfully uninstalled safetensors-0.5.3\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2024.11.6\n",
      "    Uninstalling regex-2024.11.6:\n",
      "      Successfully uninstalled regex-2024.11.6\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0.2\n",
      "    Uninstalling PyYAML-6.0.2:\n",
      "      Successfully uninstalled PyYAML-6.0.2\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 25.0\n",
      "    Uninstalling packaging-25.0:\n",
      "      Successfully uninstalled packaging-25.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.3.0\n",
      "    Uninstalling numpy-2.3.0:\n",
      "      Successfully uninstalled numpy-2.3.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.10\n",
      "    Uninstalling idna-3.10:\n",
      "      Successfully uninstalled idna-3.10\n",
      "  Attempting uninstall: hf-xet\n",
      "    Found existing installation: hf-xet 1.1.3\n",
      "    Uninstalling hf-xet-1.1.3:\n",
      "      Successfully uninstalled hf-xet-1.1.3\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.5.1\n",
      "    Uninstalling fsspec-2025.5.1:\n",
      "      Successfully uninstalled fsspec-2025.5.1\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.18.0\n",
      "    Uninstalling filelock-3.18.0:\n",
      "      Successfully uninstalled filelock-3.18.0\n",
      "  Attempting uninstall: charset_normalizer\n",
      "    Found existing installation: charset-normalizer 3.4.2\n",
      "    Uninstalling charset-normalizer-3.4.2:\n",
      "      Successfully uninstalled charset-normalizer-3.4.2\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2025.4.26\n",
      "    Uninstalling certifi-2025.4.26:\n",
      "      Successfully uninstalled certifi-2025.4.26\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.4\n",
      "    Uninstalling requests-2.32.4:\n",
      "      Successfully uninstalled requests-2.32.4\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.33.0\n",
      "    Uninstalling huggingface-hub-0.33.0:\n",
      "      Successfully uninstalled huggingface-hub-0.33.0\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.1\n",
      "    Uninstalling tokenizers-0.21.1:\n",
      "      Successfully uninstalled tokenizers-0.21.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.50.3\n",
      "    Uninstalling transformers-4.50.3:\n",
      "      Successfully uninstalled transformers-4.50.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.0 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.0 which is incompatible.\n",
      "langchain-core 0.3.63 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.5.1 which is incompatible.\n",
      "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed certifi-2025.4.26 charset_normalizer-3.4.2 filelock-3.18.0 fsspec-2025.5.1 hf-xet-1.1.3 huggingface-hub-0.33.0 idna-3.10 numpy-2.3.0 packaging-25.0 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.4 safetensors-0.5.3 tokenizers-0.21.1 tqdm-4.67.1 transformers-4.50.3 typing-extensions-4.14.0 urllib3-2.4.0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "certifi",
         "numpy",
         "packaging"
        ]
       },
       "id": "eb72f2aaeaff44d7ae178b431f215292"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!pip install transformers==4.50.3 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dfy9LNxvrkJA",
    "outputId": "dcbe705c-4fea-4aec-cb6c-203a9ead0d78"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "202506120916\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!mkdir -p /content/drive/MyDrive/ml2025_hw10\n",
    "\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "now = datetime.now(pytz.UTC)\n",
    "formatted_time = now.strftime(\"%Y%m%d%H%M\")\n",
    "print(formatted_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w50IXBsaQ9g2"
   },
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UM_b1_CXRAD7",
    "outputId": "4a59f820-e36c-4944-9232-881c4eea0658"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fatal: destination path 'ml2025-hw10' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/hsichelin/ml2025-hw10.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIXyDO66QcOs"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "h1Ysn3pEQ5CR"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import accelerate\n",
    "import numpy as np\n",
    "import safetensors\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration, set_seed\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, CLIPTextModel\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "import diffusers\n",
    "from diffusers.pipelines import BlipDiffusionPipeline\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel, DiffusionPipeline\n",
    "from diffusers.loaders import AttnProcsLayers\n",
    "from diffusers.models.attention_processor import CustomDiffusionAttnProcessor, CustomDiffusionAttnProcessor2_0\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.utils import load_image"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(transformers.__version__)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WM-2b4989EOK",
    "outputId": "e0040b5d-e0ba-47c4-ee90-89c587f28635"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4.50.3\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiLa4LZJT1Yu"
   },
   "source": [
    "## Set Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "agRpwvaFT3tB"
   },
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNqnezOrqK1X"
   },
   "source": [
    "1.先跑Method1 再跑 Method2 ，不要用 run all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FMdqaABQPpe"
   },
   "source": [
    "# Method 1: BLIP Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28juUW0WQipo"
   },
   "source": [
    "## Define Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wd9esgJSUCBb"
   },
   "outputs": [],
   "source": [
    "def InferenceBlipDiffusion(\n",
    "    pipe,\n",
    "    cond_image_path,\n",
    "    name,\n",
    "    text_prompt_input,\n",
    "    guidance_scale,\n",
    "    num_inference_steps,\n",
    "    saveDir,\n",
    "    num_images\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs inference using a Blip Diffusion pipeline to generate images based on a conditional image and text prompt.\n",
    "\n",
    "    Args:\n",
    "        pipe: The Blip Diffusion pipeline object.\n",
    "        cond_image_path (str): The file path to the conditioning image. This image guides the style and content of the generated images.\n",
    "        name (str): A descriptive name associated with the subject or concept in the conditioning image. This name is used for both the conditioning and target subjects.\n",
    "        text_prompt_input (str): The text prompt that provides additional information and guides the generation of the new images.\n",
    "        guidance_scale (float): A value controlling the influence of the text prompt on the generated images. Higher values enforce the prompt more strongly.\n",
    "        num_inference_steps (int): The number of denoising steps to perform during the diffusion process. More steps generally lead to higher quality images but take longer to generate.\n",
    "        saveDir (str): The directory where the generated images will be saved. The function will create this directory if it doesn't exist.\n",
    "        num_images (int): The number of images to generate.\n",
    "\n",
    "    Returns:\n",
    "        None. The generated images are saved to the specified `saveDir`.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(saveDir, exist_ok = True) # create output directory\n",
    "    # prepare arguments for BLIP Diffusion\n",
    "    cond_subject = name\n",
    "    tgt_subject = name\n",
    "    cond_image = load_image(cond_image_path)\n",
    "    negative_prompt = \"over-exposure, under-exposure, saturated, duplicate, out of frame, lowres, cropped, worst quality, low quality, jpeg artifacts, morbid, mutilated, out of frame, ugly, bad anatomy, bad proportions, deformed, blurry, duplicate\"\n",
    "\n",
    "    for i in range(num_images):\n",
    "        output = pipe(\n",
    "            text_prompt_input,\n",
    "            cond_image,\n",
    "            cond_subject,\n",
    "            tgt_subject,\n",
    "            guidance_scale=guidance_scale,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            neg_prompt=negative_prompt,\n",
    "            height=512,\n",
    "            width=512,\n",
    "        ).images\n",
    "        output[0].save(f\"{saveDir}/{i}.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Pp1SKZLUGji"
   },
   "source": [
    "## Create BLIP Diffusion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "5e027eee454544b38201253655b92179",
      "6bc3ca64d7644e43b8a8df22f7e45441",
      "39b57f39cda549b281c1d47af66032d0",
      "16e4d33098904e059a06cac7005a9555",
      "7ba80766335b4223bf963c3280f330a5",
      "4d2bb547c5f64705a1aab4c48f4a3a47",
      "7e6ce52ce5d2407fa79f9ee9cefe32f9",
      "73a89f3c09f449d0acdb05d91120a1c8",
      "48594a4259e24f8b86afbc4ac96469c2",
      "ee6ce095757f4c248b958b3b93dec31d",
      "f9ace4d2840248b6ab82f041d587c3af"
     ],
     "height": 0
    },
    "id": "ZMkGnQfKUKAb",
    "outputId": "ae6599bc-c3e0-4429-b229-de0e3e871ed9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "cannot get type annotation for Parameter mean of <class 'diffusers.pipelines.blip_diffusion.pipeline_blip_diffusion.BlipDiffusionPipeline'>.\n",
      "cannot get type annotation for Parameter std of <class 'diffusers.pipelines.blip_diffusion.pipeline_blip_diffusion.BlipDiffusionPipeline'>.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5e027eee454544b38201253655b92179"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "An error occurred while trying to fetch /root/.cache/huggingface/hub/models--Salesforce--blipdiffusion/snapshots/b7479a77cc10d8a51aad9638b5bd3f83a5692183/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /root/.cache/huggingface/hub/models--Salesforce--blipdiffusion/snapshots/b7479a77cc10d8a51aad9638b5bd3f83a5692183/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "An error occurred while trying to fetch /root/.cache/huggingface/hub/models--Salesforce--blipdiffusion/snapshots/b7479a77cc10d8a51aad9638b5bd3f83a5692183/vae: Error no file named diffusion_pytorch_model.safetensors found in directory /root/.cache/huggingface/hub/models--Salesforce--blipdiffusion/snapshots/b7479a77cc10d8a51aad9638b5bd3f83a5692183/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    }
   ],
   "source": [
    "blip_diffusion_pipe = BlipDiffusionPipeline.from_pretrained(\"Salesforce/blipdiffusion\", mean = None, std = None).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrpazsMLUNBF"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0,
     "referenced_widgets": [
      "82b8519a61b94fb99d5b088b6be89b4d",
      "f8d4bc3da5824656a44389c32ab77019",
      "3bfccec584c440ff8ba43a6c265c2d76",
      "11e94081742745b4aa91b8606b4a5831",
      "c0cc4791b0154bfbb8ec5e29ceac4e0a",
      "b380b215f1d24920bbdab72ec45f17f5",
      "d6d67ea4c58246d4af47c0268e16548b",
      "49fc75144a4c444a9b44f16c02e5d71a",
      "5c1d60f7c3b94421acc8495c418d4fc8",
      "63a67ec4587f48678f5c8a47a19226ca",
      "3be5ffc5623540d69d4c61d3209b48a5",
      "6923e057c2a245498c895b35e3a0671c",
      "f6f026a411104f678b65a225a21c9e26",
      "13254aa8889d4a0f9efd10ac7f18c5b1",
      "3ce66dcfda0c41aa945343c6579050c1",
      "43064a9020b64d3b99ec1fcddd2a4cb9",
      "14c92a8a0bf44f00be1f4436468b6730",
      "13f2b0cd2124420cb727924a60e1b391",
      "9f5b9f8e52cf49b7b0144b0252c4ffb6",
      "0975ae78841d451e99850bba2099273a",
      "37d4d326caca48a89b587064186ba431",
      "f9178112b866403b95050e3eea2b2150",
      "cbcf0ca2030849df8903ec2366b60d72",
      "edded0cdcda74246a8fa2aaf882e0694",
      "4c71f81d0320449485beb29f261671c2",
      "b7d1cba48a394d15a564ee6c895ec390",
      "cedcd6fafb8744178cd73c643c8faeb8",
      "40f333fcf28846a286d8c194dbc62382",
      "e1966acec6a346bab6037aa3a915ead9",
      "e7c38d79aa3d4fc683344e536c63b75d",
      "db6715ebe13548168e7cc034d30a2a2c",
      "b43614deef6c4ad7ad4e114ecc2c80b4",
      "cd08dae26a854a5da4afb04bf6cced40",
      "5196c8bc453d4f85944bef1f5f8c7bd2",
      "87a32ef2f370417c9a47051cf77c65f7",
      "e9a30fcac79b4401b1e09c660cf9657c",
      "7a9f75028e6e405eaa6f1359a286cadc",
      "298310c082c640b78182d543d88d3e75",
      "734f05be443a48dd87e606e61b2c9d4f",
      "b7c899cd8f134f95859e4fb1628369f9",
      "2a4a1b8a01d7488f9a968f8bc7ca7f38",
      "24ee7434a10e455fa905ad6c70d2806a",
      "81dcf3412abe4e589303b896095319ff",
      "be1548985f2e4d26b6aefc98897452b4",
      "ca7ccd5704694b7695e6728b9a9b1b54",
      "5970e1f3bbbb43a89ca28343e98600ec",
      "84c90f4daa194522b566632bf02ef241",
      "708d7314752d43bda8c092f240de79fb",
      "e9efe66a770f419e864fc75fb932db6b",
      "290c1210a3b54ab08fcb2561888eece9",
      "b05769720b5147319450e3c1b036d07b",
      "9afd28034af440c5ae8e2a38eaf47dc1",
      "4e255a627cd749b484abf2318704d2f9",
      "eb28cd8b15fc4e4e905ef5120ed48703",
      "18d877139ed44052b81e51e94b83d8a8",
      "3419a2fff4884289b6948d56ee6986c5",
      "4967a252048a417faf96e4c0fa614978",
      "ab2e9260b25f4bf9b89e5610f36198de",
      "1523a115d640469e83d57838316dd8a1",
      "fb232808b6644e06b2d86f2ffe178de8",
      "11b26dc768ef44c48528626c7e49968e",
      "316f799602884d0ba78c62a4f0318826",
      "2aed2ce9afe442489ee318c6128b46c7",
      "c0a5ad5627554e45a1cd91ca9cd061fc",
      "959a3f9fafcb485cad0278086e3781dc",
      "6dda43fb012d4b1392c22750aa9e9c16",
      "79250f88de7b404c8d0eefdbecd3846b",
      "5abfc37ac6f24a89bbb594bb43743c2a",
      "33a0ca529b134c6db294729ff93e6394",
      "2dc743a846694a19828a2db24e86c396",
      "10cbc4144671479e9694add0c07c53ce",
      "d9eb3deab05044dbabe54b95cbad2059",
      "2349905b189244b187b163b387e4069a",
      "d15de9e2bb7a4dd59be64885e9284170",
      "d482c7de6a5444b08335b4dcd1d51653",
      "ab342d6947934865a471ef58d9910a6c",
      "22cdbbcb2504417ba2383425f0b7ca40",
      "3e8399582f844b7dadda0f2f5b27ee8e",
      "2029dcecc3174eb1a63c5f2eb43287da",
      "557a6feba3c040179bc94643b99a7090",
      "17d9897f66db4ef193393c256a57603f",
      "3d2af20a364b490f842510190d2ab854",
      "afffd408d8ea4c268613e5c8605c7f5b",
      "064dec839e7a463688dc5d3cf4ea255c",
      "b783bc1e45374c92a05b002c91938ee0",
      "e0b2a11d8ff84dbf97bb5aa05b5fceb3",
      "daaa68c897a440f6994c84eebdacdd2b",
      "9391a5161bbe4db798821b1b07776343",
      "26b57d608bd74cf9be918dffedb54918",
      "35648bc556634632843b5315a838df18",
      "e285fe85f2724eccbf60850264cabc88",
      "2f59ea877a014aefbea7a546554df008",
      "a9691c0a586b461aa7fb6cff61969ab2",
      "1ed57d7928cf4274b1a33a2e20a2889f",
      "b7a39ca98ead45beb5719fe84d4c4087",
      "090ac111481645518e5e8ca41cb14cc7",
      "5d26d4e3484d40eea2451cf11276f661",
      "7ace0f2294ac4f548c56bfc19017a8d3",
      "86b8f3b5c0c64635b8930008f5342fc5",
      "89cf411c46ee4689a9fdb5d6970aa4ef",
      "2832e04fa97a43799cbc04cd6ccea32b",
      "b58255dd45764c98852962deca1626ca",
      "3d799a9494404358a2fc183cf017def7",
      "cda878428ec4453db9d3861f71eae722",
      "761bc9cfb3644b9096210b920eb8bdc3",
      "47ba6850e8e54dcc90bb01bef7f7bf54",
      "81a45f461fbf4e9e8565a59fa9b4f575",
      "0dc1d95f9b91460ab0fdb09cec981eff",
      "6f84ac32b04a4d9cbfedcef0bbbd3e36",
      "1cd00cd1eb354ef8804ce2e5fc23c0c4",
      "cabfd69f82fd409fbb37b9622d0e6d92",
      "cf5eed47d5b040439f0932d16b87c706",
      "24fd12d00ddd429cbd951bd6dca2f7e9",
      "f9b0c1db062449ea9981d91073b1ae33",
      "8abd4356b95b465886385a66e6ee5b26",
      "24038f1828544031a0622599a677dbb3",
      "d40ab839d7a74fa7a35a10e0b6c7ef3d",
      "d5be678bfe8a4dc18ee821470a9f2144",
      "cda00abdc1a54998b3db8c062f3707f5",
      "b4716b09323a4b2aa18304c1a2ae624b",
      "66bc50bf1a3f4c67943de4953c475078",
      "b21255becf4c416c96092242764c9906",
      "709cbe93b48e455b84bb7cf9449ae8ce",
      "1377ac5a84394ec3af33b93d21831a3d",
      "7604e4b217c143289439995af34105ec",
      "113ed672acd344f596a1660983955a6e",
      "34381facff404f429376785475eafe15",
      "d635f0701cd446cf9e6be704e2176701",
      "a0b8057a96c54f20a1b5b7642f7be5d0",
      "17c2d591d96343b29f8740f1f57ff7a5",
      "dba4f0ec4252400fb09d1f1eb27b2b63",
      "6bd587fd71e84141a731a01afcc78fec",
      "4ed1636a57c4449191943d0532b7781a",
      "a49622ee046e4d6c9d64e5d208d17c7f",
      "660214c3cf614da4890825fe5c61493c",
      "de31d92df99544a89a391828ea7bfba8",
      "892193e740574b3eb147c57b73e5003f",
      "61123b7588c547408b722302421b0873",
      "d519d205b0664e0f804f29646ed01932",
      "811fc94c06bd45d09bcf9a58f3654b58",
      "4405ae24767240f88e743a5189dad3a0",
      "38675fcf087b4140b8b8e885cec111d7",
      "920f0ad87cb64e669015c8f7d614a25d",
      "95bb6b63125c4939b71ced1c92751d5b",
      "72e2b256af3b4a19ad2e51d36a9f2a25",
      "82b382b37c1141a59ff7ecd8a6c3098e",
      "7fb077a0cdb14b9a890a529a4cd74df3",
      "de224ae5d8754e3bb286344a17c69984",
      "5a5141664eb1454bb9ba838b48863fd5",
      "b42d958bf2ab4ba78680bd3631af02fb",
      "adcec309804a449f94aacc3f9aedb175",
      "2043270133f9446b8446e399c5eaefeb",
      "f34bd460c704435aa888b829b67e7039",
      "15c87b1f2adc4aa596e042bb2074fa39",
      "870f165413f9426784b6257352a05452",
      "8053e6384a2b42159192c5a26f639efa",
      "c0a04e826a504926aec8bfa0eadc5f65",
      "725d140e912745d5ae1df7544eaff777",
      "dc9912a829aa4662ac0812c0ab8c4e0b",
      "e284404a5b57475fbf554a3d5b5f41fd",
      "c01dd412217444488e980548490756ea",
      "d8f608497a564405aeaa2783ddcb9e4c",
      "2662506d61604b58a9af97dded996a5e",
      "95ee6fd2e3b94d3087329f9c66ccd2c1",
      "03386f66f66d482f856f1e6d8c59d6cd",
      "97cffe0113464bea83279078d6a40408",
      "bc4fb5b069304dd4a5d186161760df82",
      "19001b4cffc84d908a6d350f291776cb",
      "200f65a94e674b7bb095646aea14be63",
      "7f1514777fc74dd1b1e366ccc1b1142b",
      "f9bce94fdf124459bf27a6ccee8e1d30",
      "d75f60631db543e38f59a64eae852d29",
      "7258823d7f354d208bd8611c806e9089",
      "e5a42f66a0034a37ba3378c8abdc9cb3",
      "e0ed3c980edb4bf88209d3690b0fc3d8",
      "6fe4595b6f55441798d2316071d4f7a1",
      "438c31a8ed594855bcfc6e8bea0fdd5c",
      "cd5b74da51964eee92975883aa804ef7",
      "305c62b25a9f4a3683d2b1bf35312130",
      "d7eb3fa56d7547518a35ca156c36bbf4",
      "9c6c5d007fb943e8a4f018a75640b6da",
      "7c649696a6e74246a6b555d18ed2b2c6",
      "5176e87d75d74483b7f5439dcccbfdb1",
      "cc00c1c533ed46d6bacad7c0ee71a49f",
      "f93eee61bd844562b102fbc821846927",
      "878e8ed447ea45f08adca2fbae9a2fe9",
      "849bcf33543b4c6381b530186f359b0d",
      "49ee042d08764635945a4349a2f08730",
      "91a1483cbfd34f47bf398eef475eed27",
      "24e038e56c1b419890456bae6c66dd16",
      "b2053eded0524e108c409b92b50f8d32",
      "5a494df64a234d42a9900f4c7531c748",
      "21eb308bca5e4938899d40fbf70b4b5f",
      "1f7fe45b5c934a48b6974ed2550967f1",
      "045d33f85ee2429b8fd01540d0e1671e",
      "1c7e2cf23e2742b3b1551154be83c034",
      "ca6b8f2a2baf4b42bd9d139e68e63d69",
      "81807eba416d4d08bd011712d9305505",
      "b30dd0f639cc4cc39111c006cd50b039",
      "9d6cc22da3dd4e2c80fb18b6264dee15",
      "18be116b27a84483a65f4767446dfb8f",
      "645c1ce8520f42a8a22556af0ab7f0bc",
      "fa27a314080949ad975e30c7df779af3",
      "fed9cfda0c414675960b07b4689ad041",
      "5af7bd459b394c7099af5b16f0b23b30",
      "0b6867d6880e48e4a66cc65b2c34950b",
      "6b7ee70d8c2c49a294498bdd3e88add1",
      "2381d9f08af84aa98e6aed3f737cf2da",
      "49da9af8233242a99cb1776678a926a8",
      "1ee9485abfc64fb480e6a520483625e2",
      "0ae300fa8fee42a48b63ec939ac885c7",
      "15deb21ca93342568a34dbf2e8db3a62",
      "c98d42e0a965406a8739ef6c2245103d",
      "033fb90f1e3a4dc080a15a68595c9f20",
      "2455866d229148a394eca9e13951ecd3",
      "64c5d4ab745d40de8af182fd1b437c72",
      "2e31f92d5cbf476ab9259736cedda904",
      "be10dca60c324165a13f9a518895a349",
      "4fc9b3a5a2d549b4b85af682b32ece15",
      "095835fbd49d41ccb4688216b1f87269",
      "cdf33ade21fc48b19b75a12bcb2f38d5",
      "f95a2ce70e2a4eef8217af37a667d1fc",
      "2d68a681d41a4d99ab2550282a0aaca7",
      "b6b8ad09d9a747b2aa58c8c2b89f31e8",
      "24d98cefaec24517b6147064d70dcabf",
      "e8dc3f86f88747c593f9cf73ff8f5807",
      "c9e2690f90c34dd3b984f9f2ac0ee8cc",
      "61e06af75d6a414988dd2a03a18d9407",
      "48b487c8c1ea473da460929491e260fd",
      "a8ff41cfb2574722b0ec3df1919d6e95",
      "aa0f25566c6f495191efc316b06722c9",
      "717b489275514038b75bfc20b49ce308",
      "29696d27ed7a4147ad5e0ba16be48f61",
      "7d94e1d56e734c15a19903ddb02ca8a7",
      "a91e4b83a3984b9090383f557283e85d",
      "deb7b47959d041a7a4492d2ba05fcb9c",
      "b365949ec6384f77b0a7d10ad66926ac",
      "c2101fce45ad48ad9ea4c1decd157a70",
      "974f0074780f4d7a98432f12bfead434",
      "82c7c03df05246b498a29fd8fff8d1fb",
      "7f7bf3b516f44c50a0a47c3bb51a3e53",
      "7b5b126964db42f5a123ad44bcf63357",
      "e38292455067481b8c9d6daa2c1ff902",
      "01200d1da7644377a72f232368d64553",
      "42e909b5446e43228a58c213ba1571a4",
      "9b654beb8b3c46cb981c3c21dd56dd4f",
      "783e4414d0304912820c10e392ccaf38",
      "703da0dfc5fc46988b1014fe7d1baeb1",
      "234654d4529340c2bf6d94bdc08bcdb6",
      "c14a5c36ee8644a69b706ddad6bde3ab",
      "add512889d5b457993eca19f3f29395d",
      "2586ea1458c94b64b034f3e3f7010ae9",
      "32e676aabc5343a586b632fd72b9e0f2",
      "3eaf034cf9094dba85aa11c7afe5a629",
      "6beb12eeb0a24ceaa35505342c7b447f",
      "bc02bb311ec74fd188fe9773bee662aa",
      "212e7a21d52248ffaee3bc8f2c78fecb",
      "e068b70bba344f20b98c2c483a06710a",
      "f67a840b736842afb93823755fb128f8",
      "8959f75dfc2d4cee9bb208b3e14abef7",
      "96d8b71f423a4027aa775b57bfef56be",
      "3cc03564e95d43e48cb81c985a338862",
      "79e3350e03fc4d3dbe9c355125d8387d",
      "6741a9d4206a4455864d533869082107",
      "acd220ce79114256b8d1037b78ed5291",
      "4999b6cb91d64235ae4d2c580c1e49cf",
      "fc1f8b962c4f4712940a4bb3c285e200",
      "aca83a9836f849faa7a038682e7f8f63",
      "0a336dba576543d58a6674e1b8024a69",
      "793cf73d650f4a1782fc70cdf134c6ab",
      "02f4cd1f09cb4ba8905583ababad31e0",
      "c2427cfff76445278c673640b979eeea",
      "91886e25f6954271a31cbbe443c49d94",
      "df5e236c22eb481ba4100c9cf0acd146",
      "78d76840373f40bab1d6a0dddefcfbd3",
      "67fc6975d7504bdd97018477f8e328ed",
      "8982fa194951495199d1d8236e95c0cc",
      "9584ffce2315470fb458170b3c90b7e8",
      "999faee629634cd2abc38cdf458b5c7a",
      "7ecac56a01554762bd01acdd1dc78b95",
      "649e5226b7c946aaa128bab8120d426e",
      "3f8dfe68fdf746f48f30776dadea9326",
      "0d7867b11a2c41ea8a8e097794d01dd5",
      "c296722f53574ddd87744a119b755397",
      "9f0ce068247244eba6bf5c8a417bdaba",
      "b7e89c4f4575414caccfd1c59d8a1e94",
      "eb0ca506c9eb4351b6993f63794b8426",
      "6b4c8bfb37ce4b5e98b11eb2ac710a7e",
      "763044957c654606896d45284a32ea6f",
      "57946de3aa724590936e6c34edeae16a",
      "123ec5761e374f8d9fbc41f89fb4cb51",
      "6abd322b8615440cbc65eafefef5802d",
      "99f6ef8e0e224baabdf4afbf4becdb89",
      "fac58cd4709646a3bfa2550e4a8af6a7",
      "c0d837b6d67f44a494288624f4f1a401",
      "f4932cfcdf1a443b960d72aeb939f21d",
      "e64725bcb13343b58b788984e8c5f645",
      "f8faca82863b4df5bcf929f7b214522f",
      "7f67e9c2fbec4ed7bcf4fe53d6353fa0",
      "5c5de2b0750e4693a7e5e391c240d958",
      "3744da710de6401ebc1ef5379b53a051",
      "3671ccaec07548d2a99904a0328f106b",
      "b0ceeb177e2a4eb7b2fad689b8c7a4e5",
      "72934d43399b4f50817eed1213637119",
      "b8d7054bdfec47b48fe9a130ad092a3b",
      "5c17c2bc5607465985a4224ffb3d4974",
      "810293147ab14dc7a9114d5aaa335fe7",
      "4cb62401f71c4685ba85044fe7ee6803",
      "f9059672c4164f5e952e601ec9bffec3",
      "1b9fee75fd03413691bf6b6655e5e56f",
      "82e1443b55324c648e682e1d8bfb8631",
      "5b57e8170b9b41d8a081242e663a1cfb",
      "1168e780c7d7409b92f6170877f281d0",
      "9b40809918224a93943b0d5291f74876",
      "663eef91c6c049f7bcdd80679763c217",
      "d8bb582b40b74cab8711a18dcacbb393",
      "2104295035784e6281043df672b22756",
      "1e0c059354d44255b773e42552a73ecb",
      "e6d4558945504aadab11cdff7cccac40",
      "e3427c0f952444058e7175850d83781b",
      "5299383124a0441a8b40c3bc42855583",
      "5bd191e073234ee3878312b5a1787bd8",
      "1c58e723f04e4ff7baca6d7017664099",
      "741cb097f061476d921dda45ed196dbd",
      "046566bcb54c4456b28b7ca916774e39",
      "f3ffbbfed6024572aaa3359aed58b01c",
      "fbd1b394f31b4e398c34cb3fc3819e73",
      "b0e32d24ca604655bbe0ab2c9b7806f5",
      "52228eb116dd47869af114a898519ec0",
      "731fc2f0fbf94b00b3521d8a6db4a7c8",
      "ec9b3687b4fc43dbab914085ce655fac",
      "b8bf5b67ac414601a1fb3554e05c9304",
      "315208b9d8124f2c80a4eeb929464985",
      "8e6c91db56f641f4b73821acdf913b78",
      "0edf52fdafec45dfb94943778b93d5cf",
      "855bf11c3f1243e289fad0dcac11ed38",
      "66ba6de8b1994bec902496814c308536",
      "954eb15b233f42b5bc7bd0d2a13cd2ae",
      "6609ae51c162422ea5809e22649611dc",
      "1a5bfdde12354c6e80e7c4090c609100",
      "883165ef7034445387b67aa17832ed9e",
      "859f345d13774efaa956b4fa1a227a4c",
      "dc27b79770f24549858171a62f5d0566",
      "f7e0dcf3a9744033bf187c268387dda6",
      "0a788e8d98104e358453b58c75cea52c",
      "b571f3868d364733afa62a16342a3a18",
      "f5122d49c46e4e95b7ab8499720602af",
      "f0d0960b5bb64d61837e604d8833a514",
      "ac28c57532b047818461714afedc8357",
      "513dc96ef706485fb437c3bbbcf84e06",
      "71c36b827eec45c084d9155873c5dbf5",
      "4500f6f989fb4ee58998d3e24f091855",
      "1e6c509d4ab44f3991b6aa39905ef28b",
      "23b7fc23681d4297b4545994d603768d",
      "482d42ca79a345b2b19e2033c0c6215e",
      "bef5396b7c2441d8ad2d50674c3fe4b7",
      "eb5e7957720d4e64b4f5a2e30f78571d",
      "fc2d97a5c9664d64851ee541351c190d",
      "fcfb4a753ba644f8bdfe784c43b904f8",
      "91bdedb891ce4fa3b6fe720d0dc2f7a4",
      "b03aa6bad45f4a158cb5dd238fd4778f",
      "05bb6c350c0146c4a6f51dd075339048",
      "6d08ab12ae9e47a7aae114a1f1d8d61e",
      "b8a39ebb21b9414a87ca031d33e6ebd1",
      "a5e64354ec45458b81de67bc46f1e415",
      "5d869ea72e5447e7a1bb44dcddba9453",
      "a537da705c2442dc81f5b5bcd70ebe1b",
      "3277ccc729d948268c3f7ddcfacc57b0",
      "e3a0c95c97be4376a4c6aa6c63f6c3ea",
      "9693328889a544028bb3ba4e44ac7a90",
      "c510d8d595c2454bb029b485673867c0",
      "661a6937632e44c5b10a4028527b9d21",
      "b164e84e5d864f009e466cf0f119673c",
      "7628b311159b4302a42baea1ed8896b1",
      "626a3126d5624c068b93fa01f41d57ef",
      "b0a2ff78d5e846808a93123a6de3d611",
      "70f39d4319e14571a911bccc79c6f71f",
      "23e2f2e83e4040f0a39ee2390902c2d9",
      "cc2c12a97f7848d8861309322bf40f9e",
      "a5dc94b09c704e78bb5f64736b79f719",
      "1be4944980b34a1a95d9622014f971fd",
      "d394a10b8d864e80b89c69a3231455f4",
      "cdc732f056554429a1aefdc830af7e20",
      "11c28940b6844625913524d52d68cc7e",
      "e00490b704c94701a3bd830094ddb7ae",
      "e68315e48ace4906bfd2b8563480d920",
      "008d8d106ad646ebb08bab141d22674d",
      "3a784470a9ea4e46922f79608d8d43dd",
      "d2e6b68c0e4e44b6bf9f704922d3a2c9",
      "8ada503c95cd484ab2da3063d7e8339e",
      "9f84e38caf164bc381136b8f6bd5ec94",
      "e0250e31a9274a7db8c562462aee29a6",
      "76613af462d24c2998b8caf80b498bc3",
      "f06e5c4468dd40a49c1e36753102836c",
      "6e246cc3b2db415595760fc0f6a5fd73",
      "a28fca632bb345d6a69bffbdc630f4e1",
      "069c014cb0e6443ba6541f5399381338",
      "f8457c6360ea49178226e10646067128",
      "cec9c63ac24c44cb80d2a29f3951aa15",
      "7ff26320f52c437aa4983c872e44e6d5",
      "179c640834664f9f8ff7a5320cc69ab2",
      "008976a0667848c2afab418f90463b91",
      "02cd64ffd42b486094855eb98b982f82",
      "592a6dd424ad4ec098a529fde69ece2d",
      "61638a5c31734856b67a759646c99ffd",
      "c782e9f5e1ca4ceda66dfed78c50a506",
      "5801b3ab65f944bfb7cec1951ef38455",
      "f68a60a9808d4b0583d0b25b54498d51",
      "de9cc21dbe3e4749bb9f08bfe63e578d",
      "30d4cf75e69a4de98dabefa6d1251b08",
      "113afd1b00b449f19d644b343694dfe6",
      "c8f695de084d4d398a4129ee0d6f4a34",
      "3ed865f8c9a54e32bd9ba585ab86a715",
      "8cd15ab6d8ba4a0aa054dda260dbd384",
      "eba968f5a1fd46878621269dd626ccac",
      "e4ef7e484cda4a67904e0047dfec9d70",
      "3f0acc3708294a059329b00850972298",
      "63fbb25244144c629781b5d095fba859",
      "e24166bc0027448a8bfd272269267d52",
      "bc3b2cdef6d847d48a7757fc5b07a54e",
      "9d33a4eb4d2a4c10b864cc372caab1d5",
      "848b52be808b45509f90e0f96e2c50c2",
      "3715d3dd683d41f0a64fa7eb1b492325",
      "5afed4c50c214b9b810a16b95ae87900",
      "565ff32bc7ab4c3494486864dc5eeaf4",
      "a5f206d696dc4b68a95b749814a56fa5",
      "e1d4a2eb77ae4c74bef222dee37ee502",
      "129d574c37d049938294a1e870517ba8",
      "6ad2184c5e0d478aaf907b99a6fa95c4",
      "abcd99cfae704ade83c9c8929ccb618c",
      "3faf398b93834d40a4b332d1dcb31d43",
      "2435e3e35a7b4ec8a3f0eee8c993a5e3",
      "b7c8d44fb96d4fa5b729824acaa6aca0",
      "44956d1b057b431a9766a539a5f4ba85",
      "9ddd047f3c724fc4bc5ffad508b9cc76",
      "210b800d790e4840bc52b474079d816e",
      "0a4fafa5afe743d28d0cad994eeac946",
      "ef2d3e4a91184409807a2ceee191f51b",
      "8b39c452b4bd469487ed29a0524f1387",
      "0e26a447df7d4887bfba9cc87dec1be7",
      "90601969c5b74720b40a91df6ebdae8e",
      "6e65b32aedae41d59a221d2627efd0bf",
      "5249724ca5af44809f0a6dd0b446dd0f",
      "d4716519ec1849a08fb2f9a684bdd0a4",
      "1f1293113205495f93e07075acb2185d",
      "12489f4e8cd04f8fa9b6d062656666d4",
      "c47b3477ba3242e78e5254cce72ae1a5",
      "24efc447598749048ded301a87ef365b",
      "01369d7b14c346819c869fe36d3c2773",
      "b1c7b408c84a486588dd61739a8ef308",
      "cf7531a4c35f4e0cb2b4de818aa5e5d5",
      "6f2900b09f7e45b48560ad2febdc4fbf",
      "bc5abc101d184a999e4367a9b9ee6e1d",
      "5b24bd4687194daf81f6e38a7455b32d",
      "09eb0a977b994b03ad45606785fc49cf",
      "a423915704434b1d9937abcb1fa32fe4",
      "6ca16d8902df4fd8988a6117e9318f06",
      "f83a63c82a6e45d8a69551b2f66d367d",
      "362685d81869420fbbb64f123d6dd11f",
      "339b1bd241e6475ba20351804eb25b89",
      "ba7bd763121648c68f461c5395819515",
      "f6852d6f0cca44cdb36b1f2406f1d590",
      "c9e23f71efbd4b1c8ab2a1a93f6f3377",
      "b4c683fc5580418f8d690025499de0d3",
      "6e1e8ea376984434942c8338e2c360ad",
      "ad78c92748c44668a5db979492e316dd",
      "40982e398f0d4783a46af029895b8d9a",
      "217ebaf4365541e781ecad986636efff",
      "a60b440987e3496a82ce86112dbd9a95",
      "edee279323d54274a4f16d8ead87f133",
      "98ae2d5b5b1b4be9b9b697cfbf0a2735",
      "b7587785b5eb478da67f6fe0d252940f",
      "0a7defbd63e14f8c83f009143a2213cc",
      "e30a64e5c7804d588caa88cb8d956463",
      "2d3d27ff43ad402e8e7e01a0a24f4e66",
      "da0cdd9b7a404bcf8f6a4131afca2577",
      "8f2ee436d97e40dc9105342de5f35d9b",
      "bee63390f8c149359f7daa3e9a6660c5",
      "9742a6679bad4562b83448853a0d77ca",
      "122d65694ea44a73936e18fe6ece422e",
      "49870eff985c46f081bc62d0aa797f1d",
      "3665f0fe77d34b9bba4b41cd78adc38a",
      "c40dfaf74057485b994a5a5c522a4c0c",
      "22d4a4f5f54346f8baccb62d7fe846cd",
      "2abf2565eaa8458ca6f2847a29eed3ec",
      "5e64973d01d44baab9509ef48ed8d743",
      "e8948e5115864678833a3d1180a693e9",
      "5a2fbca3916349be8e75c78466355f52",
      "d94a548982bf4912ae44eae58a9c6ff6",
      "0cfcd598c77546129ed471d9f1a70950",
      "beb0be3f95be47ee9e109956bd93dc76",
      "b623de5bd5cd479d98d9b9bdf73603de",
      "c398c9323c8b4a3099afd2511315a9e3",
      "42a4742af2c84e2d878b5754f565fe22",
      "9705553d88ab4ff5a0a8377df2eabda0",
      "2ab15bc7eab2474eb0747d48ed030b53",
      "92c632e6bba84210b5a0dc173b3ea8d6",
      "a2c818de0bc54a05a1b99741d3049899",
      "042baf3a36d846e2b5838067a700fa7a",
      "fcf61baf5c2e4d7cb68971fa5aa5c729",
      "05fe1e54a1c843369b47f613204dd823",
      "ccebd349ba074f32a1968db671f8144d",
      "9b9b03751b744030aecd9ecdf3976735",
      "3472c2d6856c48d396185f8a83efafdf",
      "c979b3193e9d4a40af6e1f0b74e0afd7",
      "302478534b414432bd4b4ef06e46d116",
      "e1c2ca9ffc3742dc8a0db0169dc97811",
      "4d8841db97204cbda221d792dec07660",
      "cad480e726e44158a1f7cbc453b28b94",
      "e3f84f241a6d4a45b64eb7f9cf55b724",
      "941350f9e58d4a32b4ad00f3e41d9446",
      "493998bd31c042a5a6731ef8e3a14b7b",
      "d5ce63ea05fa412a82baf34c3b72d0b1",
      "173d78c8a68346619e42256b8221a402",
      "2ca6d0c672f544049f143aa92b3e196e",
      "612a190dde3c42bc9f6edd7f33a876be",
      "65a484e3345e40b3b98ed67bc666b785",
      "33360c0e706549f7a89b644845963eb1",
      "ef02ebe2d2ca4ad1b0d0f533fc2b7fd3",
      "44e9a72eabdb48a680f49b36dd444117",
      "1f8475f6a9ac473994caac1a19b343e5",
      "6ca4eaa14f1746f4b967016c4edf69fb",
      "633a5b2aa4f246598691f58734bf143d",
      "0254d90618e74556ab14da2842a028be",
      "152406b8d79d42edb9a22ebf12c5e288",
      "f1023a16d98d441e8af92f8db57f88b6",
      "ad1dc19534ad4af586eaf54cf6a3fb56",
      "1c5fda269d8f4784841c0d4059742272",
      "3322816edb124dfc842a1419d82af589",
      "2e80688ee4bf4ba09b8903367d27d32e",
      "6649d53121724ff4bbf416f88d039b35",
      "7012ad619828478288bb0c53ac5cedbe",
      "8af214d950e54d429cc82ef2e0dbe8a1",
      "796284b8ab5e40bcbd688c0b7b724a99",
      "1433433fe39b4f2c952c621228bc5652",
      "bf9efa3af95f49b9ac752e8ffe5e4c63",
      "5745256f9b364e5988ffc21cd78821fa",
      "e70674f85d174b758323d7bfac2874eb",
      "7671ec7d02634b2aa798379165ea9248",
      "3f44024bbd8f45e39e389130509040dc",
      "d99480529201490e956098d6c78e2287",
      "242bae82afa645e9904d5aae1d5d6ae3",
      "d7ebb404c9f84f2a9365590be2ce68c7",
      "72077e4df6764077a5caa7ee6deb70f8",
      "6395dcfd1eba404c8f51d9916e0871d1",
      "c0f1008f673a413896a664912c652fef",
      "9e7266b3f5504d4fadf29f88b0981f25",
      "1a2e83eb091f4caa8a009a6c76064aae",
      "faa6411466c54f5489744c21263dfaa3",
      "3392d3b44e504fa1b3abda3070ad49e4",
      "ae70e45fb478483899aa900a67327e77",
      "de6265835136463c938bc511a2b73b53",
      "43b0ed92285e4b3087982bbc039c7646",
      "09e66616e6fb4728a3394de143390e70",
      "6a6655678dbf45538e7c3e94bbfd3dd2",
      "c53c89362f9944099fd9383d21459296",
      "d76f295ca1af493c88430209538d513e",
      "417256858c5e4be7998f32f28cf4e1ee",
      "03fcc60a144240019dfa1bff49ac71f2",
      "57be5d4bae6647c4bd79c06bf7e022d4",
      "1a2a41a6d7df43a8a05a5b8165ab451e",
      "9d4067efacce4908b1fc1b5bbcefbf91",
      "253472c57526414bbdd0ddea78f47eb4",
      "ee2876e4930b449bb2a52bdc2dbd0038",
      "d41a0cf442c04dc98095d7a93a38f749",
      "f2e77e2ccf3f41f0866f4b3c835d26ef",
      "61a9dd0f4ba9460ba4b17a1544bac7fb",
      "04c38fb5c74348a6a4013f41aba7eeb0",
      "621090c7181347a6bb1c92a054f459b8",
      "f5d6d022f4934096a0cf03f202bb41c1",
      "d529e3e2c98a4dda94d0aed8dc0b5183",
      "7f174bdd73a2408c9f2e337b33654388",
      "a0333c0fd0c6433cb6b89f439c9ad110",
      "6c33331fdc594231aada122ab4590804",
      "739ae0f4799d402cb2ef6cb763a1c0cb",
      "0bcecfd8a2994f6b9e19d52f74d45148",
      "23c9d9c028424cd0bee38d7237543a8b",
      "0d2b45f13eb442db8d6629aecad0258e",
      "665988912c604c51b81b7ef12d60a836",
      "9c23c29ffeff4925933cdd327ad34471",
      "7ca31e2585e0455bb9363b1a8fab9037",
      "58a515ce593840e19f953fd3e08eed1b",
      "864ebfc69db24bc08708d70bc14d5851",
      "ac300fbbe4884b02a73c8db646e18ec6",
      "ec0f8126cd4049dba8a500b79ab15adb",
      "4b10e2998ef44fe39c0383f0c23c2bea",
      "842306c436b349d7a869ae02c18f7760",
      "454274b9056846a9aa069fda15ddc716",
      "e4d69a8b8f7a47ebbd353c6f6202bb56",
      "1530023148de4aeba94a1a4f89fd12d6",
      "a1a9fc1a958e42dcab2d0504aff44db5",
      "66064e923bcf4a0ead69fcf237cb377c",
      "b99790cec7cd46bc80ebefd71f9c709d",
      "8f6b57ad165741bf9fecfd0b24553655",
      "2a7b9abba9434c189b0174ef5bfa3ed9",
      "e3d902227818409b82b871c9d066f4a8",
      "d656008f1851417992b6500e4c2a790d",
      "c1b3a19e99ca4bda8397026a1f04c43b",
      "2b4a5f4b04f643ec8e3ac5086225e8a5",
      "57e512240a8f4b92b89b6030712adaa0",
      "52548d89459a4d6687a9a54f33709da1",
      "08879ba09fe64610859bad920edf96c2",
      "0a34e3666fb84fb191460d89c11c81b1",
      "ed91cd27db6b44d4a6f05be69ac955fd",
      "60d3c799badb4739b0fecf91b59c1cf1",
      "a8427412479f420897b2f81e1fc03902",
      "657c403b7dd544a1b09a072a3d503db0",
      "57a879d67ea244448f656385d2bd3e04",
      "edc707fce5934e8b8c5cd89ee53ff45c",
      "3ff1a60a7d864a5388cb9ed32a8b49b9",
      "945d79ac5552485da526aa2e8bc06345",
      "6a3688f88cbf489a8a3337603c07744b",
      "c857f84d58f446878a298a11f9467300",
      "564a2eebed264b5e8cd58595340359fb",
      "a939398ab1d94878b7bfdd997ec78065",
      "0fed74adf21d4f54aad954fb942b5691",
      "39ff898f8f9d412ba87292d9c1a7ae29",
      "0b7da24caa1840e08de4f8065006e6cd",
      "4cd63f201690436d984dbee90995a7f8",
      "b5e4db79430741bda8c5e99f4c79abe6",
      "4ceacb0afde84d279d8d5227c2bb8d89",
      "20f9fdc1b1b54175aff71de4d29a98c9",
      "02e5b17411cc423eaaf399685c27a78d",
      "f740151694774ac8a257162817b563aa",
      "d76e567fe92a4b3d9e0f01d9d55d458e",
      "e7c4686bd773487e81f2ee7fc18cb44e",
      "7fa4afc3a8e14f9fb23a9b6e59b3e31b",
      "3fe225ccafa14794804dac64128dab52",
      "cb45fd80f6314b649f9026c4982c95ba",
      "6c674fb00c8249ccbf0cba45b46c87c1",
      "f88b40ee1ddb44b9a16d1625e16629eb",
      "7fb2f24c72bc455eabfe349b86bc59e9",
      "f857d049613742c8a466a570373fabb4",
      "86c50f082c9a4e46b5a15332e7754dc5",
      "64942f94e9df47c29fb51417dbed6c4e",
      "b57ed29e48a34a1ea95f22d48ff6a0e0",
      "62be465323e54314be7b0c073f705e14",
      "85f51c5ddcfa4ed89c272f938666a2dd",
      "d7590a89b6f24786a2b84f0386baf1ab",
      "c5aa6013ac464280aa889d576b48f933",
      "1008edeb7f2d4988b3d3c1cf90dc544d",
      "9b034ef19e18432e83d46112bf5eedf2",
      "c33cbbc325e3468c9b65c1fb9faced4f",
      "97abb51ff17647e69b80ac70ea9aec66",
      "0954a6bec6e94c38ad0b0ddeaca5088a",
      "54936c4447eb443da44c950bbaf90a8e",
      "fa5d0479e2b6479dafab65572de15278",
      "b8fd1d11a9b349898c5a273825f9c97e",
      "0ce5907775f34b1c8852f65df06c6ccb",
      "2612856127f74d84961447c37e1824e2",
      "6dd2b886e14d46fe9c772293a2b1e50f",
      "92506cad9e4445bc977d42e49220a3cb",
      "8f26dda7b4f84156b794447dd0d8312e",
      "d2d2ee97f6514bd3b5d77d65ceff180c",
      "adb6dc856cd240a9ad610127236a6519",
      "594c0086731c44f88918191eedebeb30",
      "946e75dcef104ba7a992ba4de13c17c0",
      "b88d1438d187462d8a623bbdd3a783de",
      "6661919ba9a84ee89e6d2746e0bb5c2b",
      "f75fd9450bf744c4938c1499eac7acf0"
     ]
    },
    "id": "nq3fexPiUO8G",
    "outputId": "1f67f1bd-67ce-4e77-ec05-237649919a9b"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "82b8519a61b94fb99d5b088b6be89b4d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6923e057c2a245498c895b35e3a0671c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cbcf0ca2030849df8903ec2366b60d72"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5196c8bc453d4f85944bef1f5f8c7bd2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca7ccd5704694b7695e6728b9a9b1b54"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3419a2fff4884289b6948d56ee6986c5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "79250f88de7b404c8d0eefdbecd3846b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3e8399582f844b7dadda0f2f5b27ee8e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26b57d608bd74cf9be918dffedb54918"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "89cf411c46ee4689a9fdb5d6970aa4ef"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cabfd69f82fd409fbb37b9622d0e6d92"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b21255becf4c416c96092242764c9906"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4ed1636a57c4449191943d0532b7781a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "95bb6b63125c4939b71ced1c92751d5b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "870f165413f9426784b6257352a05452"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "97cffe0113464bea83279078d6a40408"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "438c31a8ed594855bcfc6e8bea0fdd5c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49ee042d08764635945a4349a2f08730"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b30dd0f639cc4cc39111c006cd50b039"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1ee9485abfc64fb480e6a520483625e2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cdf33ade21fc48b19b75a12bcb2f38d5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "717b489275514038b75bfc20b49ce308"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e38292455067481b8c9d6daa2c1ff902"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3eaf034cf9094dba85aa11c7afe5a629"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "acd220ce79114256b8d1037b78ed5291"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "67fc6975d7504bdd97018477f8e328ed"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eb0ca506c9eb4351b6993f63794b8426"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f8faca82863b4df5bcf929f7b214522f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f9059672c4164f5e952e601ec9bffec3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e3427c0f952444058e7175850d83781b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec9b3687b4fc43dbab914085ce655fac"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "859f345d13774efaa956b4fa1a227a4c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1e6c509d4ab44f3991b6aa39905ef28b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b8a39ebb21b9414a87ca031d33e6ebd1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "626a3126d5624c068b93fa01f41d57ef"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e68315e48ace4906bfd2b8563480d920"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "069c014cb0e6443ba6541f5399381338"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f68a60a9808d4b0583d0b25b54498d51"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e24166bc0027448a8bfd272269267d52"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "abcd99cfae704ade83c9c8929ccb618c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "90601969c5b74720b40a91df6ebdae8e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f2900b09f7e45b48560ad2febdc4fbf"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c9e23f71efbd4b1c8ab2a1a93f6f3377"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e30a64e5c7804d588caa88cb8d956463"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2abf2565eaa8458ca6f2847a29eed3ec"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2ab15bc7eab2474eb0747d48ed030b53"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e1c2ca9ffc3742dc8a0db0169dc97811"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "33360c0e706549f7a89b644845963eb1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3322816edb124dfc842a1419d82af589"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f44024bbd8f45e39e389130509040dc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae70e45fb478483899aa900a67327e77"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d4067efacce4908b1fc1b5bbcefbf91"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a0333c0fd0c6433cb6b89f439c9ad110"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac300fbbe4884b02a73c8db646e18ec6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a7b9abba9434c189b0174ef5bfa3ed9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a8427412479f420897b2f81e1fc03902"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "39ff898f8f9d412ba87292d9c1a7ae29"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3fe225ccafa14794804dac64128dab52"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d7590a89b6f24786a2b84f0386baf1ab"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2612856127f74d84961447c37e1824e2"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "with open(\"ml2025-hw10/metadata.json\", \"r\") as f:\n",
    "    objects = json.load(f)\n",
    "\n",
    "##################### TODO: Tune hyperparameters here ##########################\n",
    "\n",
    "num_inference_steps = 150  #less than 100  # The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\n",
    "guidance_scale = 7.5     #less than 20   # Higher guidance scale encourages to generate images that are closely linked to the text prompt, usually at the expense of lower image quality.\n",
    "\n",
    "################################################################################\n",
    "\n",
    "num_images = 15             # The number of images you want to generate.\n",
    "                            # WARNING: You MUST to keep it 15 if you want to generate the images for submission.\n",
    "                            # But you can reduce it when tuning hyperparameters to speed up the process\n",
    "output_dir = f\"/content/drive/MyDrive/ml2025_hw10/results_Method1_{num_inference_steps}_{guidance_scale}_{formatted_time}\"\n",
    "\n",
    "# iterate through each of the 6 objects to customize\n",
    "for (obj, info) in objects.items():\n",
    "    # If you only want to generate results for specific object remove the \"#\" below and adjust the list\n",
    "    #if (obj not in [\"object-1\", \"object-2\", \"object-3\", \"object-4\", \"object-5\", \"object-6\"]): continue\n",
    "    if (obj not in [\"object-1\", \"object-2\", \"object-3\", \"object-4\"]): continue\n",
    "    InferenceBlipDiffusion(\n",
    "        pipe = blip_diffusion_pipe,\n",
    "        cond_image_path = info[\"path\"],\n",
    "        name = info[\"name\"],\n",
    "        text_prompt_input = info[\"text_cond\"],\n",
    "        guidance_scale = guidance_scale,\n",
    "        num_inference_steps = num_inference_steps,\n",
    "        saveDir = os.path.join(output_dir, obj),\n",
    "        num_images = num_images\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zxdvPl0VPuI"
   },
   "source": [
    "## Archive Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hZBACEytVTZk",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c18d5085-3be6-416c-a17e-1dc7ed8219d6"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "os.system(f\"zip -r {output_dir}.zip {output_dir}\")      # create zipped file for submission (make sure you generate 15 images for 5 objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDAyYMLMs1t9"
   },
   "source": [
    "需要重新連線 才可以在runn，然後重新跑 除了Method1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jf3qzTRjQUCg"
   },
   "source": [
    "# Method 2: Custom Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R06LX1gfVotL"
   },
   "source": [
    "## Define Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Z7Xo4VmIQbCN"
   },
   "outputs": [],
   "source": [
    "class CustomDiffusionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset to prepare the instance and class images with the prompts for fine-tuning the model.\n",
    "    It pre-processes the images and the tokenizes prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        concepts_list,\n",
    "        tokenizer,\n",
    "        size=512,\n",
    "        mask_size=64,\n",
    "        center_crop=False,\n",
    "        with_prior_preservation=False,\n",
    "        num_class_images=200,\n",
    "        hflip=False,\n",
    "        aug=True,\n",
    "    ):\n",
    "        self.size = size\n",
    "        self.mask_size = mask_size\n",
    "        self.center_crop = center_crop\n",
    "        self.tokenizer = tokenizer\n",
    "        self.interpolation = Image.BILINEAR\n",
    "        self.aug = aug\n",
    "\n",
    "        self.instance_images_path = []\n",
    "        self.class_images_path = []\n",
    "        self.with_prior_preservation = with_prior_preservation\n",
    "        for concept in concepts_list:\n",
    "            inst_img_path = [\n",
    "                (x, concept[\"instance_prompt\"]) for x in Path(concept[\"instance_data_dir\"]).iterdir() if x.is_file()\n",
    "            ]\n",
    "            self.instance_images_path.extend(inst_img_path)\n",
    "\n",
    "            if with_prior_preservation:\n",
    "                class_data_root = Path(concept[\"class_data_dir\"])\n",
    "                if os.path.isdir(class_data_root):\n",
    "                    class_images_path = list(class_data_root.iterdir())\n",
    "                    class_prompt = [concept[\"class_prompt\"] for _ in range(len(class_images_path))]\n",
    "                else:\n",
    "                    with open(class_data_root, \"r\") as f:\n",
    "                        class_images_path = f.read().splitlines()\n",
    "                    with open(concept[\"class_prompt\"], \"r\") as f:\n",
    "                        class_prompt = f.read().splitlines()\n",
    "\n",
    "                class_img_path = list(zip(class_images_path, class_prompt))\n",
    "                self.class_images_path.extend(class_img_path[:num_class_images])\n",
    "\n",
    "        random.shuffle(self.instance_images_path)\n",
    "        self.num_instance_images = len(self.instance_images_path)\n",
    "        self.num_class_images = len(self.class_images_path)\n",
    "        self._length = max(self.num_class_images, self.num_instance_images)\n",
    "        self.flip = transforms.RandomHorizontalFlip(0.5 * hflip)\n",
    "\n",
    "        self.image_transforms = transforms.Compose(\n",
    "            [\n",
    "                self.flip,\n",
    "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def preprocess(self, image, scale, resample):\n",
    "        outer, inner = self.size, scale\n",
    "        factor = self.size // self.mask_size\n",
    "        if scale > self.size:\n",
    "            outer, inner = scale, self.size\n",
    "        top, left = np.random.randint(0, outer - inner + 1), np.random.randint(0, outer - inner + 1)\n",
    "        image = image.resize((scale, scale), resample=resample)\n",
    "        image = np.array(image).astype(np.uint8)\n",
    "        image = (image / 127.5 - 1.0).astype(np.float32)\n",
    "        instance_image = np.zeros((self.size, self.size, 3), dtype=np.float32)\n",
    "        mask = np.zeros((self.size // factor, self.size // factor))\n",
    "        if scale > self.size:\n",
    "            instance_image = image[top : top + inner, left : left + inner, :]\n",
    "            mask = np.ones((self.size // factor, self.size // factor))\n",
    "        else:\n",
    "            instance_image[top : top + inner, left : left + inner, :] = image\n",
    "            mask[\n",
    "                top // factor + 1 : (top + scale) // factor - 1, left // factor + 1 : (left + scale) // factor - 1\n",
    "            ] = 1.0\n",
    "        return instance_image, mask\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        instance_image, instance_prompt = self.instance_images_path[index % self.num_instance_images]\n",
    "        instance_image = Image.open(instance_image)\n",
    "        if not instance_image.mode == \"RGB\":\n",
    "            instance_image = instance_image.convert(\"RGB\")\n",
    "        instance_image = self.flip(instance_image)\n",
    "\n",
    "        # apply resize augmentation and create a valid image region mask\n",
    "        random_scale = self.size\n",
    "        if self.aug:\n",
    "            random_scale = (\n",
    "                np.random.randint(self.size // 3, self.size + 1)\n",
    "                if np.random.uniform() < 0.66\n",
    "                else np.random.randint(int(1.2 * self.size), int(1.4 * self.size))\n",
    "            )\n",
    "        instance_image, mask = self.preprocess(instance_image, random_scale, self.interpolation)\n",
    "\n",
    "        if random_scale < 0.6 * self.size:\n",
    "            instance_prompt = np.random.choice([\"a far away \", \"very small \"]) + instance_prompt\n",
    "        elif random_scale > self.size:\n",
    "            instance_prompt = np.random.choice([\"zoomed in \", \"close up \"]) + instance_prompt\n",
    "\n",
    "        example[\"instance_images\"] = torch.from_numpy(instance_image).permute(2, 0, 1)\n",
    "        example[\"mask\"] = torch.from_numpy(mask)\n",
    "        example[\"instance_prompt_ids\"] = self.tokenizer(\n",
    "            instance_prompt,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids\n",
    "\n",
    "        if self.with_prior_preservation:\n",
    "            class_image, class_prompt = self.class_images_path[index % self.num_class_images]\n",
    "            class_image = Image.open(class_image)\n",
    "            if not class_image.mode == \"RGB\":\n",
    "                class_image = class_image.convert(\"RGB\")\n",
    "            example[\"class_images\"] = self.image_transforms(class_image)\n",
    "            example[\"class_mask\"] = torch.ones_like(example[\"mask\"])\n",
    "            example[\"class_prompt_ids\"] = self.tokenizer(\n",
    "                class_prompt,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.tokenizer.model_max_length,\n",
    "                return_tensors=\"pt\",\n",
    "            ).input_ids\n",
    "\n",
    "        return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqD1K8I8Vx34"
   },
   "source": [
    "## Define Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "GKtCY6xcV9ZF"
   },
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    \"\"\"\n",
    "    Puts together a batch of data.\n",
    "\n",
    "    Args:\n",
    "        examples (list of dict): A list of dictionaries, where each dictionary\n",
    "            contains the keys \"instance_prompt_ids\", \"instance_images\", and \"mask\".\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the batched \"input_ids\", \"pixel_values\", and \"mask\".\n",
    "              \"input_ids\" is a concatenated tensor of prompt token IDs.\n",
    "              \"pixel_values\" is a stacked tensor of instance images.\n",
    "              \"mask\" is a stacked tensor of masks with an added channel dimension.\n",
    "    \"\"\"\n",
    "    input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n",
    "    pixel_values = [example[\"instance_images\"] for example in examples]\n",
    "    mask = [example[\"mask\"] for example in examples]\n",
    "    input_ids = torch.cat(input_ids, dim=0) # Concatenate prompt token IDs along the batch dimension.\n",
    "    pixel_values = torch.stack(pixel_values) # Stack individual image tensors to form the image batch.\n",
    "    mask = torch.stack(mask) # Stack individual mask tensors to form the mask batch.\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float() # Ensure data is in contiguous memory for potentially faster processing and convert to float.\n",
    "    mask = mask.to(memory_format=torch.contiguous_format).float() # Ensure mask is in contiguous memory and convert to float.\n",
    "\n",
    "    batch = {\"input_ids\": input_ids, \"pixel_values\": pixel_values, \"mask\": mask.unsqueeze(1)} # Add a channel dimension to the mask to be compatible with image processing layers.\n",
    "    return batch\n",
    "\n",
    "def save_new_embed(text_encoder, modifier_token_id, accelerator, modifier_token, output_dir, safe_serialization=True):\n",
    "    \"\"\"Saves the new token embeddings learned by the text encoder.\n",
    "\n",
    "    Args:\n",
    "        text_encoder: The trained text encoder model.\n",
    "        modifier_token_id (list of int): List of token IDs corresponding to the modifier tokens.\n",
    "        accelerator: The accelerator object used for distributed training.\n",
    "        modifier_token (list of str): List of the modifier tokens (e.g., [\"<new1>\"]).\n",
    "        output_dir (str): The directory where the new embeddings will be saved.\n",
    "        safe_serialization (bool, optional): Whether to use safe serialization (safetensors). Defaults to True.\n",
    "    \"\"\"\n",
    "    # Unwrap the potentially distributed text encoder model to access its components.\n",
    "    learned_embeds = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight\n",
    "\n",
    "    # Iterate through the modifier tokens and their corresponding IDs.\n",
    "    for x, y in zip(modifier_token_id, [modifier_token]):\n",
    "        # Create a dictionary to store the learned embedding for the current modifier token.\n",
    "        learned_embeds_dict = {}\n",
    "        # Extract the learned embedding for the specific modifier token ID.\n",
    "        learned_embeds_dict[y] = learned_embeds[x]\n",
    "\n",
    "        # Determine the filename and saving method based on the safe_serialization flag.\n",
    "        if safe_serialization:\n",
    "            filename = f\"{output_dir}/{y}.safetensors\"\n",
    "            # Save the learned embedding dictionary using the safetensors format.\n",
    "            safetensors.torch.save_file(learned_embeds_dict, filename, metadata={\"format\": \"pt\"})\n",
    "        else:\n",
    "            filename = f\"{output_dir}/{y}.bin\"\n",
    "            # Save the learned embedding dictionary using the standard PyTorch format.\n",
    "            torch.save(learned_embeds_dict, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_qe8NscWFTJ"
   },
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "iTZufl9qWIU2"
   },
   "outputs": [],
   "source": [
    "def train_func(\n",
    "    output_dir,\n",
    "    instance_prompt,\n",
    "    instance_data_dir,\n",
    "    freeze_model,\n",
    "    learning_rate,\n",
    "    max_train_steps,\n",
    "    train_batch_size\n",
    "):\n",
    "    accelerator_project_config = ProjectConfiguration(project_dir=output_dir, logging_dir=Path(output_dir, \"logs\"))\n",
    "\n",
    "    # create accelerator\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=1,\n",
    "        mixed_precision=None,\n",
    "        log_with=\"tensorboard\",\n",
    "        project_config=accelerator_project_config,\n",
    "    )\n",
    "\n",
    "    # Disable AMP for MPS.\n",
    "    if torch.backends.mps.is_available():\n",
    "        accelerator.native_amp = False\n",
    "\n",
    "    # We need to initialize the trackers we use, and also store our configuration.\n",
    "    # The trackers initializes automatically on the main process.\n",
    "    if accelerator.is_main_process:\n",
    "        accelerator.init_trackers(\"custom-diffusion\")\n",
    "\n",
    "    # Handle the repository creation\n",
    "    if accelerator.is_main_process and (output_dir is not None):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    pretrained_model_name_or_path = \"CompVis/stable-diffusion-v1-4\"\n",
    "\n",
    "    # create tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        subfolder=\"tokenizer\",\n",
    "        revision=None,\n",
    "        use_fast=False,\n",
    "    )\n",
    "\n",
    "    # Load scheduler and models\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "    text_encoder = CLIPTextModel.from_pretrained(\n",
    "        pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=None, variant=None\n",
    "    )\n",
    "    vae = AutoencoderKL.from_pretrained(\n",
    "        pretrained_model_name_or_path, subfolder=\"vae\", revision=None, variant=None\n",
    "    )\n",
    "    unet = UNet2DConditionModel.from_pretrained(\n",
    "        pretrained_model_name_or_path, subfolder=\"unet\", revision=None, variant=None\n",
    "    )\n",
    "\n",
    "    # Adding a modifier token which is optimized ####\n",
    "    modifier_token_id = []\n",
    "    initializer_token_id = []\n",
    "    modifier_token = \"<new1>\"\n",
    "    initializer_token = \"ktn\"\n",
    "\n",
    "    # Add the placeholder token in tokenizer\n",
    "    num_added_tokens = tokenizer.add_tokens(modifier_token)\n",
    "    if num_added_tokens == 0:\n",
    "        raise ValueError(\n",
    "            f\"The tokenizer already contains the token {modifier_token}. Please pass a different\"\n",
    "            \" `modifier_token` that is not already in the tokenizer.\"\n",
    "        )\n",
    "\n",
    "    # Convert the initializer_token, placeholder_token to ids\n",
    "    token_ids = tokenizer.encode([initializer_token], add_special_tokens=False)\n",
    "\n",
    "    # Check if initializer_token is a single token or a sequence of tokens\n",
    "    if len(token_ids) > 1:\n",
    "        raise ValueError(\"The initializer token must be a single token.\")\n",
    "\n",
    "    initializer_token_id.append(token_ids[0])\n",
    "    modifier_token_id.append(tokenizer.convert_tokens_to_ids(modifier_token))\n",
    "\n",
    "    # Resize the token embeddings as we are adding new special tokens to the tokenizer\n",
    "    text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # Initialise the newly added placeholder token with the embeddings of the initializer token\n",
    "    token_embeds = text_encoder.get_input_embeddings().weight.data\n",
    "    for x, y in zip(modifier_token_id, initializer_token_id):\n",
    "        token_embeds[x] = token_embeds[y]\n",
    "\n",
    "    # Freeze all parameters except for the token embeddings in text encoder\n",
    "    params_to_freeze = itertools.chain(\n",
    "        text_encoder.text_model.encoder.parameters(),\n",
    "        text_encoder.text_model.final_layer_norm.parameters(),\n",
    "        text_encoder.text_model.embeddings.position_embedding.parameters(),\n",
    "    )\n",
    "    for param in params_to_freeze:\n",
    "        param.requires_grad = False\n",
    "\n",
    "    vae.requires_grad_(False)\n",
    "    if modifier_token is None:\n",
    "        text_encoder.requires_grad_(False)\n",
    "    unet.requires_grad_(False)\n",
    "\n",
    "    # For mixed precision training we cast the text_encoder and vae weights to half-precision\n",
    "    # as these models are only used for inference, keeping weights in full precision is not required.\n",
    "    weight_dtype = torch.float32\n",
    "    if accelerator.mixed_precision == \"fp16\":\n",
    "        weight_dtype = torch.float16\n",
    "    elif accelerator.mixed_precision == \"bf16\":\n",
    "        weight_dtype = torch.bfloat16\n",
    "\n",
    "    # Move unet, vae and text_encoder to device and cast to weight_dtype\n",
    "    if accelerator.mixed_precision != \"fp16\" and modifier_token is not None:\n",
    "        text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "    unet.to(accelerator.device, dtype=weight_dtype)\n",
    "    vae.to(accelerator.device, dtype=weight_dtype)\n",
    "\n",
    "    attention_class = (\n",
    "        CustomDiffusionAttnProcessor2_0 if hasattr(F, \"scaled_dot_product_attention\") else CustomDiffusionAttnProcessor\n",
    "    )\n",
    "\n",
    "    # now we will add new Custom Diffusion weights to the attention layers\n",
    "    # It's important to realize here how many attention weights will be added and of which sizes\n",
    "    # The sizes of the attention layers consist only of two different variables:\n",
    "    # 1) - the \"hidden_size\", which is increased according to `unet.config.block_out_channels`.\n",
    "    # 2) - the \"cross attention size\", which is set to `unet.config.cross_attention_dim`.\n",
    "\n",
    "    # Let's first see how many attention processors we will have to set.\n",
    "    # For Stable Diffusion, it should be equal to:\n",
    "    # - down blocks (2x attention layers) * (2x transformer layers) * (3x down blocks) = 12\n",
    "    # - mid blocks (2x attention layers) * (1x transformer layers) * (1x mid blocks) = 2\n",
    "    # - up blocks (2x attention layers) * (3x transformer layers) * (3x down blocks) = 18\n",
    "    # => 32 layers\n",
    "\n",
    "    # Only train key, value projection layers if freeze_model = 'crossattn_kv' else train all params in the cross attention layer\n",
    "    train_kv = True\n",
    "    train_q_out = False if freeze_model == \"crossattn_kv\" else True\n",
    "    custom_diffusion_attn_procs = {}\n",
    "\n",
    "    st = unet.state_dict()\n",
    "    for name, _ in unet.attn_processors.items():\n",
    "        cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "        if name.startswith(\"mid_block\"):\n",
    "            hidden_size = unet.config.block_out_channels[-1]\n",
    "        elif name.startswith(\"up_blocks\"):\n",
    "            block_id = int(name[len(\"up_blocks.\")])\n",
    "            hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "        elif name.startswith(\"down_blocks\"):\n",
    "            block_id = int(name[len(\"down_blocks.\")])\n",
    "            hidden_size = unet.config.block_out_channels[block_id]\n",
    "        layer_name = name.split(\".processor\")[0]\n",
    "        weights = {\n",
    "            \"to_k_custom_diffusion.weight\": st[layer_name + \".to_k.weight\"],\n",
    "            \"to_v_custom_diffusion.weight\": st[layer_name + \".to_v.weight\"],\n",
    "        }\n",
    "        if train_q_out:\n",
    "            weights[\"to_q_custom_diffusion.weight\"] = st[layer_name + \".to_q.weight\"]\n",
    "            weights[\"to_out_custom_diffusion.0.weight\"] = st[layer_name + \".to_out.0.weight\"]\n",
    "            weights[\"to_out_custom_diffusion.0.bias\"] = st[layer_name + \".to_out.0.bias\"]\n",
    "        if cross_attention_dim is not None:\n",
    "            custom_diffusion_attn_procs[name] = attention_class(\n",
    "                train_kv=train_kv,\n",
    "                train_q_out=train_q_out,\n",
    "                hidden_size=hidden_size,\n",
    "                cross_attention_dim=cross_attention_dim,\n",
    "            ).to(unet.device)\n",
    "            custom_diffusion_attn_procs[name].load_state_dict(weights)\n",
    "        else:\n",
    "            custom_diffusion_attn_procs[name] = attention_class(\n",
    "                train_kv=False,\n",
    "                train_q_out=False,\n",
    "                hidden_size=hidden_size,\n",
    "                cross_attention_dim=cross_attention_dim,\n",
    "            )\n",
    "    del st\n",
    "    unet.set_attn_processor(custom_diffusion_attn_procs)\n",
    "    custom_diffusion_layers = AttnProcsLayers(unet.attn_processors)\n",
    "    accelerator.register_for_checkpointing(custom_diffusion_layers)\n",
    "\n",
    "    # rescale learning rate\n",
    "    learning_rate = learning_rate * train_batch_size * accelerator.num_processes\n",
    "\n",
    "    # Optimizer creation\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        itertools.chain(text_encoder.get_input_embeddings().parameters(), custom_diffusion_layers.parameters())\n",
    "        if modifier_token is not None\n",
    "        else custom_diffusion_layers.parameters(),\n",
    "        lr=learning_rate,\n",
    "        betas=(0.9, 0.999),\n",
    "        weight_decay=1e-2,\n",
    "        eps=1e-8\n",
    "    )\n",
    "\n",
    "    # Dataset and DataLoaders creation:\n",
    "    concepts_list = [\n",
    "        {\n",
    "            \"instance_prompt\": instance_prompt,\n",
    "            \"class_prompt\": None,\n",
    "            \"instance_data_dir\": instance_data_dir,\n",
    "            \"class_data_dir\": None,\n",
    "        }\n",
    "    ]\n",
    "    resolution = 512\n",
    "\n",
    "    train_dataset = CustomDiffusionDataset(\n",
    "        concepts_list=concepts_list,\n",
    "        tokenizer=tokenizer,\n",
    "        size=resolution,\n",
    "        mask_size=vae.encode(\n",
    "            torch.randn(1, 3, resolution, resolution).to(dtype=weight_dtype).to(accelerator.device)\n",
    "        ).latent_dist.sample().size()[-1],\n",
    "        center_crop=False,\n",
    "        hflip=True,\n",
    "        aug=True, #原本是Fasle\n",
    "    )\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda examples: collate_fn(examples),\n",
    "        num_workers=2,\n",
    "    )\n",
    "\n",
    "    # Scheduler and math around the number of training steps.\n",
    "    # Check the PR https://github.com/huggingface/diffusers/pull/8312 for detailed explanation.\n",
    "    num_warmup_steps_for_scheduler = 0\n",
    "    num_training_steps_for_scheduler = max_train_steps * accelerator.num_processes\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"constant\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=num_warmup_steps_for_scheduler,\n",
    "        num_training_steps=num_training_steps_for_scheduler,\n",
    "    )\n",
    "\n",
    "    # Prepare everything with our `accelerator`.\n",
    "    if modifier_token is not None:\n",
    "        custom_diffusion_layers, text_encoder, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "            custom_diffusion_layers, text_encoder, optimizer, train_dataloader, lr_scheduler\n",
    "        )\n",
    "    else:\n",
    "        custom_diffusion_layers, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "            custom_diffusion_layers, optimizer, train_dataloader, lr_scheduler\n",
    "        )\n",
    "\n",
    "    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
    "    num_update_steps_per_epoch = len(train_dataloader)\n",
    "    # Afterwards we recalculate our number of training epochs\n",
    "    num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "    # Train!\n",
    "    global_step = 0\n",
    "    first_epoch = 0\n",
    "    initial_global_step = 0\n",
    "\n",
    "    # progress bar\n",
    "    progress_bar = tqdm(\n",
    "        range(0, max_train_steps),\n",
    "        initial=initial_global_step,\n",
    "        desc=\"Steps\",\n",
    "        # Only show the progress bar once on each machine.\n",
    "        disable=not accelerator.is_local_main_process,\n",
    "    )\n",
    "\n",
    "    for epoch in range(first_epoch, num_train_epochs):\n",
    "        unet.train()\n",
    "        if modifier_token is not None:\n",
    "            text_encoder.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            with accelerator.accumulate(unet), accelerator.accumulate(text_encoder):\n",
    "                # Convert images to latent space\n",
    "                latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample()\n",
    "                latents = latents * vae.config.scaling_factor\n",
    "\n",
    "                # Sample noise that we'll add to the latents\n",
    "                noise = torch.randn_like(latents)\n",
    "                bsz = latents.shape[0]\n",
    "                # Sample a random timestep for each image\n",
    "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
    "                timesteps = timesteps.long()\n",
    "\n",
    "                # Add noise to the latents according to the noise magnitude at each timestep\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "                # Get the text embedding for conditioning\n",
    "                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
    "\n",
    "                # Predict the noise residual\n",
    "                model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "                # Get the target for loss depending on the prediction type\n",
    "                if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "                    target = noise\n",
    "                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
    "\n",
    "                mask = batch[\"mask\"]\n",
    "                loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"none\")\n",
    "                loss = ((loss * mask).sum([1, 2, 3]) / mask.sum([1, 2, 3])).mean()\n",
    "                accelerator.backward(loss)\n",
    "                # Zero out the gradients for all token embeddings except the newly added\n",
    "                # embeddings for the concept, as we only want to optimize the concept embeddings\n",
    "                if modifier_token is not None:\n",
    "                    if accelerator.num_processes > 1:\n",
    "                        grads_text_encoder = text_encoder.module.get_input_embeddings().weight.grad\n",
    "                    else:\n",
    "                        grads_text_encoder = text_encoder.get_input_embeddings().weight.grad\n",
    "                    # Get the index for tokens that we want to zero the grads for\n",
    "                    index_grads_to_zero = torch.arange(len(tokenizer)) != modifier_token_id[0]\n",
    "                    for i in range(1, len(modifier_token_id)):\n",
    "                        index_grads_to_zero = index_grads_to_zero & (\n",
    "                            torch.arange(len(tokenizer)) != modifier_token_id[i]\n",
    "                        )\n",
    "                    grads_text_encoder.data[index_grads_to_zero, :] = grads_text_encoder.data[\n",
    "                        index_grads_to_zero, :\n",
    "                    ].fill_(0)\n",
    "\n",
    "                if accelerator.sync_gradients:\n",
    "                    params_to_clip = (\n",
    "                        itertools.chain(text_encoder.parameters(), custom_diffusion_layers.parameters())\n",
    "                        if modifier_token is not None\n",
    "                        else custom_diffusion_layers.parameters()\n",
    "                    )\n",
    "                    accelerator.clip_grad_norm_(params_to_clip, 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad(set_to_none=False)\n",
    "\n",
    "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.update(1)\n",
    "                global_step += 1\n",
    "\n",
    "                if accelerator.is_main_process and (global_step == max_train_steps):\n",
    "                    save_path = os.path.join(output_dir, f\"checkpoint-{global_step}\")\n",
    "                    accelerator.save_state(save_path)\n",
    "\n",
    "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "\n",
    "            if global_step >= max_train_steps:\n",
    "                break\n",
    "\n",
    "    # Save the custom diffusion layers\n",
    "    accelerator.wait_for_everyone()\n",
    "    if accelerator.is_main_process:\n",
    "        unet = unet.to(torch.float32)\n",
    "        unet.save_attn_procs(output_dir, safe_serialization=True)\n",
    "        save_new_embed(\n",
    "            text_encoder,\n",
    "            modifier_token_id,\n",
    "            accelerator,\n",
    "            modifier_token,\n",
    "            output_dir,\n",
    "            safe_serialization=True,\n",
    "        )\n",
    "\n",
    "    accelerator.end_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1g1qMK4WNGx"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "9O8NyqKqWO3a"
   },
   "outputs": [],
   "source": [
    "##################### TODO: Tune hyperparameters here ##########################\n",
    "object=f\"object-{6}\"\n",
    "#instance_prompt = \"photo of a <new1> toy\"           # The text prompt used for training\n",
    " #也要改成 object-6 # Path to images of the object to customize\n",
    "instance_prompt = \"high quality photo of a <new1> plushie\"          # The text prompt used for training\n",
    "instance_data_dir = f\"ml2025-hw10/data/{object}\"\n",
    "parameter_to_train = \"crossattn_kv\"                 # \"crossattn_kv\" only train the K V in cross attention. Change this to \"crossattn\" if you also want to train Q\n",
    "learning_rate = 1e-5\n",
    "max_train_steps = 500\n",
    "train_batch_size = 3\n",
    "\n",
    "################################################################################\n",
    "\n",
    "ckpt_dir = \"output\"                               # directory name to save checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpnK349mWSdF"
   },
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0,
     "referenced_widgets": [
      "6a5704749e5b4ca1b100a8531023953f",
      "eaf28e62655e43249f8ce8fe7de6f45f",
      "abb16b38a8d547b5a09afdb059489d98",
      "80a6e32f6607424cbc675c3a5c7170a0",
      "e1f9cfa50fb8498981955aece398aa6d",
      "7a707357fc974194951a68bbb3a7cd66",
      "05e3000478794d438f875429dcbfc187",
      "f62ec30363d34f228d3f0276c0e06469",
      "c174b2eecc8a4d95a0d3024532c5a621",
      "2e9f82f03bdf4b2aa0a3449c5915a0b0",
      "7365ef21398c4bf2a28ad0ae882b294d"
     ]
    },
    "id": "ICk8w4vOWTzG",
    "outputId": "ffe0c2c9-0251-469d-d428-8daafbbce94b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Launching training on one GPU.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Steps:   0%|          | 0/500 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6a5704749e5b4ca1b100a8531023953f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Safetensors does not support saving dicts with non-tensor values. The following keys will be ignored: dict_keys(['down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor', 'down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor', 'down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor', 'down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor', 'down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor', 'down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor', 'up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor', 'up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor', 'up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor', 'up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor', 'up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor', 'up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor', 'up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor', 'up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor', 'up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor', 'mid_block.attentions.0.transformer_blocks.0.attn1.processor'])\n"
     ]
    }
   ],
   "source": [
    "accelerate.notebook_launcher(train_func, args=(ckpt_dir, instance_prompt, instance_data_dir, parameter_to_train, learning_rate, max_train_steps, train_batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KIZx86vWV02"
   },
   "source": [
    "## Load Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "N8R7siyLWZ0E",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0,
     "referenced_widgets": [
      "e65f63a2a6ca4492a778fe2cc7a1f0ca",
      "23ba316f070a4c8fa5b974ffc56117bb",
      "bc9cd3c565e94069a915947d6fb814dc",
      "dc35919d74594cafa3c370779e8a180e",
      "16c736c443ab4c138ef50ec225df5062",
      "448026561c224411aba3a4b6ac5baf5d",
      "2636e047e13e43d08ab4aac1025384c8",
      "bc07998fe9944dc7b32942b74e745f56",
      "1779038589ac4b298a3e321f78c19287",
      "be5ee167b1b9468799275911724cf0b6",
      "0fc6275d1d9c4c6890f8a1fb3312df14"
     ]
    },
    "outputId": "e89b37e6-b373-4c22-8155-791ada16ef8a"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e65f63a2a6ca4492a778fe2cc7a1f0ca"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "state_dict = load_file(os.path.join(ckpt_dir, \"pytorch_custom_diffusion_weights.safetensors\"), device = \"cpu\")\n",
    "custom_attn = pipe.unet._process_custom_diffusion(state_dict=state_dict)\n",
    "attn_procs = pipe.unet.attn_processors\n",
    "attn_procs.update(custom_attn)\n",
    "pipe.unet.set_attn_processor(attn_procs)\n",
    "pipe.unet.to(dtype = pipe.unet.dtype, device = pipe.unet.device)\n",
    "\n",
    "pipe.load_textual_inversion(ckpt_dir, weight_name=\"<new1>.safetensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGgUfU9QWttS"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "J_tOtf40Wwex",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0,
     "referenced_widgets": [
      "a657377cadea4af6a1c8eceda097bf53",
      "82899a136f4b4f06b1890f62edb96cdf",
      "9957b66766c545419ced32e9d3a4344e",
      "447f40ccd6ee48358234787faa377a45",
      "01010815b8514513b2b6eec5e0b6ff08",
      "5c832b6f476b4c3389ebd6e1be3e3365",
      "841db4d7c2d647f3a4dfa5456d39772f",
      "f48aef4bc94642f2a2c077d615c69487",
      "c794a26cbdd24b9baee29ac5a4ab823d",
      "dd58aa022aa04cab83bf32c5c0b8f164",
      "adddb4c3637e4d0b9115ecfd4d40bede",
      "1f16f6cc78874434b70b9eab8c4b1c29",
      "0ba309c6b39041238b37bcc6347e10fe",
      "7aa6b03415774ad890e7fd892081c630",
      "afdc3606b9ac482f80b6ad9c273a78a8",
      "c8674502507f41d69c901253dc5c73af",
      "2258a05b15084ffab933bc8659c7c0d8",
      "0f3d851fd4944c149118ae6e5896cfbd",
      "37e0c049c50c4a658bd745646d95279c",
      "c65868ce060c4f82adf627d63319ffd2",
      "1adfc3ec6afc42b19eba1ffdd8fd356f",
      "bee65aeb0e104f08af61455653c74de5",
      "da6055c50e7740328ae4a5c5bdb1f373",
      "5634678eb5c34f2985f95a4e8de36c04",
      "160ea6fd23874a0580a0bfb1d10276df",
      "6056f0418995440cbb39e6b3f7234870",
      "20db29f7214b40508ea924f02ac25334",
      "71fad1989104460fa697c195d8fc0d63",
      "8c061d3cb19f417abf627c456ad275f8",
      "f82ad5a3c4a64dcc84482b528b5a13f1",
      "7a3510db168a40d08ccfc88d09b5ab22",
      "9f25929945ed4c63b50a2d74ffe073a8",
      "fd27f3d2779543a7aa312aac2b704c5e",
      "f26f37b35db741f1beb24efa3ffe48af",
      "d349eab870f64ade9404ddfbc6366d19",
      "d58380e330b54e018383eacf46902e60",
      "99e06333ce984982a3e6e500249ce39c",
      "3518e4e38c5347769d2d35f57954d951",
      "c6c1d59959834bafa35903d81d06530c",
      "220711580e87441caba52b0f8be08a7d",
      "68c50024a7ff4be6a8c54c5f922b1a3c",
      "19fb374f21ee440f935b03694631a569",
      "190e6bcbe80a4080a9e4620e0892ba73",
      "000c16d9e37a4c37af02cb08a127806c",
      "e38a9c18d7114c14a529d0efcf1b8ce4",
      "b7d903a0657c4673bc9e1640fa720172",
      "7e6a7311897c41c5914e174ad29eed09",
      "495de35f9d2d47ed95d1cac7a6cde24c",
      "3b02b5ca9c9f4dbe904dbc1da24b1d27",
      "86dc2fd248b54ea1a38b8da0cba4215f",
      "1b3169b37a6946a7beb60f062ea64a8a",
      "29410d5a687444e69c31ba5186327b90",
      "6dc8d548975d4563abb0edb42193b968",
      "f67ed43454bd4834b819b86da8b409dc",
      "c3fcbd13993442c6844aff37ed5806cc",
      "2bb339828a294223ad4469d6414d4d60",
      "15fd951bb0fe4f58bef47d506a1f5012",
      "61b83621aee7426bbcdd614639843efa",
      "8ceaa8cfb9a94a25969ce1d2829f9837",
      "cdb724bdf2a1483faf02bc440aa229cd",
      "618934ea3cdd4678a8c6d2c354a15467",
      "cf1d5db79de64b878cb20182ef345412",
      "ca7e02bab4a44fcc9efc2edbf3a01ac6",
      "7000de1d9f9c4cfa99449c95b4d31886",
      "f0206e2985274e49986e792790cfea27",
      "4f118c7c29604a259b4f296f6d814e0b",
      "503ef628fa1c458da410f6a3b90b0ad8",
      "b0c0f207e0de4339988e84f93a5a9841",
      "a61f57bea81c447ca236dfc7d46b4768",
      "2b8e42ab7b4141dc91f6d05a2cab45ca",
      "784d2f6341684d299923e0143dbfbf94",
      "f2b8b38baf514ee09e6ff11965328239",
      "1ee16d51c2cd4651b8c727d565ecc804",
      "4196799e5a68430a8c6c4f6fee145512",
      "5c12501aba1447e881b55be4c5a2be2c",
      "f48719b129334178b0475e70695c8843",
      "a2f1121dc8784279bb7c766b4e5cf3c7",
      "8d6d70c4973c4462830d2f2fe39bcfe9",
      "fb48051f556b45aa8445a7e17a6d173e",
      "b47ff534d2314c1bbfb8d78691e55a9a",
      "b65c97fe0d9540b7b2369c19e21f456e",
      "9f90ba43258844d5a9d1ed334efc7172",
      "9d38260dd58641d79b20fff4c05088d2",
      "bd57615d34574193bba378867a80457f",
      "f4ce97c8dcfd4ffebd40fd55d617b741",
      "59ee43bc649b4c14a0d56214b5ebd593",
      "4fecca41195c46f1a14b39c584407575",
      "f947468384de47198ffff74a2abf60d3",
      "c292d94212914172a5d852181a5a5bd8",
      "8f310a4e3e7c47279857ef42675a0849",
      "3cd5a2cf147e4211a87fe5e9d084e709",
      "2ae569723b6b40309f48166993a44c29",
      "ff77e47f948b467cb0b332eca8b340ef",
      "2786ad18a6ea407d8d4d175d7faeef13",
      "e082193e11b54daf848397904c99df0e",
      "bcb1e89b954242af9ecba2fdc6e62eda",
      "70d028a4a9a14141a45b0a29d3477eb7",
      "7f141df43fc144cbac06626b5a7781ed",
      "e6e07704fbd248e182b6da3938417698",
      "b430e15d23c14402a134fa23d6a758e4",
      "f356247178e44b7a943f73526ec75893",
      "0b643f72bdaa43c18bdce88823807f85",
      "24154e77fdda43ccaa54ff9944a860e9",
      "ee5fe77c3e5b4b66bbef6391dde77e6a",
      "54e9c37c833a4debb9afe09b8428eeba",
      "7e454f3d06b74b4aaa75fb12dbc7e81c",
      "3bafc1f67df048dba24f2410ea4c22f6",
      "f26ea619f85e4d07a11efccdc92a5b35",
      "4362380d199d4f96b3a0f863ef6bffb7",
      "ffd726b4948342648f592e7f96c9237b",
      "318a938c0d00413eb60474ccdb707e54",
      "d73ce5c702af4f98847e2a6ce4db1671",
      "04ed9edbd6da4fc8862fe04cafeb5e24",
      "3a85fc8561c14566aeecdb60bf760b44",
      "b9b0bc8d0d894dd5a2af5b5e0e3e27bd",
      "966c1f1d5b724bf89c884577ecccd96f",
      "0654137db60948349f0f3bbff29b3f69",
      "dc7943fef2f744d9ac5f5e6e5b4b9fc5",
      "4a452243ba3c47edbc965d0716a8696c",
      "132bb31147bd4fd9b056c647633cbefe",
      "276de07f84944e29a566094550f9d17c",
      "e7a37c6494c54de7b90e1932efd2858b",
      "09118699b0404abda7242404c6565087",
      "b9f89e3563374c4bbee8e11a6c6f90c9",
      "740160208b074ad5a2b8fca4aa98264d",
      "074e32891bbd4f8682d1f23de27536e8",
      "f9b6f4d119df4153a92799b9e607fdc5",
      "d5ad43443c7f40d1ba074dc095bd6959",
      "e667a5bee5d54a4f9d18f948ac05388c",
      "fdca173043d74fc89b3d5c704b68b275",
      "cd14de8660df4245a602811cf42b1d5b",
      "f9569f42efc141e3825b0a99a42bfd01",
      "6ab6b8dae6c542c79298a749771047bf",
      "7e2f70d40dae4d458550075dca728833",
      "4219bbc2eed145089389176132aeede6",
      "270c9b2b90a04db0a2c795d8b5db2ea6",
      "ebfe049ab0cf47f2a13c92cb0bd2916c",
      "3e8058231c8c40248930cc693988d40b",
      "c4de1de61b744ce0b33ba13b6109fcc5",
      "23340cd9ad384f2483ab48876118d80a",
      "76e05ea6cfb04d5db86e62ad3e85cb26",
      "fa403779af5b48cd8ccc08079ab1aac8",
      "0879e97900734a649558b275e9f585f1",
      "67ba7ef4a58b49078f489e594fdf7bb0",
      "f845e5aca27a4d829aa676a106154fb1",
      "c4f20b5282c84f2583dd896cebba022e",
      "3d90380c9d94449e8b4520497b23ae61",
      "abe0957772f04109bb615a528d84d10e",
      "76f6a32e19e64b8683da7b8ad4bd8c63",
      "9b1cc468e5264af995ec799cc7689f14",
      "da4cbc4ba2af4d308b881ce009686152",
      "cb7f85d7be4142daad61198b5eedbe57",
      "cd0984affb184aeda9d66cf4eaa4e631",
      "d2e534d9f9cb4398b2c49a786b016467",
      "2f8fa461ce3b4461a2af7a646dec3b95",
      "b025c51da87e41ffb1f280e5871fd1af",
      "3da623bb49eb4543aea951e5011b5b96",
      "48c8b2d6c32b4268b6a305e4383e0d30",
      "64a0944d8e74487dbadcdfb9911f113d",
      "5e6f341c1d5c48eba287b2a8797a27d6",
      "cdf8480799684df2929a68012c7491db",
      "293089cea3864c0cb64ee5cf0b9163e7",
      "bbe7ddd082ab4136833ac5bed3b1410f",
      "7121d3716b1c4e20b773d06512e140fc",
      "1f8c6e2ae4a34e4ebc1282c63db2474a"
     ]
    },
    "outputId": "1d6581c7-8b06-48ea-b068-451928a910b5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "202506121027\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a657377cadea4af6a1c8eceda097bf53"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f16f6cc78874434b70b9eab8c4b1c29"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "da6055c50e7740328ae4a5c5bdb1f373"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f26f37b35db741f1beb24efa3ffe48af"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e38a9c18d7114c14a529d0efcf1b8ce4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2bb339828a294223ad4469d6414d4d60"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "503ef628fa1c458da410f6a3b90b0ad8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8d6d70c4973c4462830d2f2fe39bcfe9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c292d94212914172a5d852181a5a5bd8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b430e15d23c14402a134fa23d6a758e4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "318a938c0d00413eb60474ccdb707e54"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e7a37c6494c54de7b90e1932efd2858b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ab6b8dae6c542c79298a749771047bf"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "67ba7ef4a58b49078f489e594fdf7bb0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2f8fa461ce3b4461a2af7a646dec3b95"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "now = datetime.now(pytz.UTC)\n",
    "formatted_time = now.strftime(\"%Y%m%d%H%M\")\n",
    "print(formatted_time)\n",
    "##################### TODO: Tune hyperparameters here ##########################\n",
    "\n",
    "#generate_prompt = \"a <new1> toy in the snow\"     #text prompt to generate images (make sure the condition is correct if you are not customizing for object5)\n",
    "generate_prompt = \"high quality image of a <new1> plushie on a plate\"\n",
    "#根據下載的metadata.json內的object text_cond修改          #WARNING: if you modified \"instance_prompt\" for training you would like to modify this correspondingly\n",
    "                              ##e.g., If your instanace_prompt is \"photo of a <new1> structure\" you would like to use \"a <new1> structure in the snow\"\n",
    "\n",
    "num_inference_steps = 250    # The number of denoising steps. More denoising steps usually lead to a higher quality image at the expense of slower inference.\n",
    "guidance_scale = 16.0         # Higher guidance scale encourages to generate images that are closely linked to the text prompt, usually at the expense of lower image quality.\n",
    "\n",
    "################################################################################\n",
    "\n",
    "obj = instance_data_dir.split(\"/\")[-1]\n",
    "output_dir = f\"/content/drive/MyDrive/ml2025_hw10/results_Method2_{object}_{parameter_to_train}_{learning_rate}_{max_train_steps}_{num_inference_steps}_{guidance_scale}_{formatted_time}\"\n",
    "os.makedirs(f\"{output_dir}/{obj}\", exist_ok = True)\n",
    "\n",
    "for i in range(15):\n",
    "    image = pipe(\n",
    "        generate_prompt,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        eta=1.0,\n",
    "    ).images[0]\n",
    "    image.save(f\"{output_dir}/{obj}/{i}.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_H5GE90cXBdz"
   },
   "source": [
    "## Archive Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Pm2wrY1tXBFu",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "44de2fca-55ad-4c4b-8bf9-175d9f9cb62f"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "os.system(f\"zip -r {output_dir}.zip {output_dir}\")      # create zipped file for submission (make sure you generate 15 images for 5 objects)"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "92vxgR_ZX_d9"
   },
   "execution_count": 29,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "w50IXBsaQ9g2",
    "BIXyDO66QcOs",
    "tiLa4LZJT1Yu",
    "-FMdqaABQPpe",
    "28juUW0WQipo",
    "R06LX1gfVotL"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}